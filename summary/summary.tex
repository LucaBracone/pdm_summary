\documentclass{scrreprt}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage[style=authoryear, maxbibnames=99]{biblatex}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{multirow}
\addbibresource{bib.bib}

\usepackage{tikz}

\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\cof}{cof}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\argmin}{argmin}
\newcommand{\iid}{\stackrel{\textmd{i.i.d.}}{\sim}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\1}{\mathds{1}}
\newcommand{\powl}{^{(l)}}
\newcommand{\inv}{^{-1}}
\newcommand{\Np}{\textsc{N}_p}
\newcommand{\Nor}{\textsc{N}}
\newcommand{\sns}{\textsc{sns}}
\newcommand{\Bern}{\textsc{Bern}}
\newcommand{\Beeta}{\textsc{Beta}}
\newcommand{\Exp}{\textsc{Exp}}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\nindep}{{\rotatebox[origin=c]{90}{$\nvDash$}}}

\author{Luca Bracone}
\title{Bayesian joint inference of multiple graphical models using
spike-and-slab priors}

\begin{document}
\maketitle

\section{Introduction}
\subsection{Motivation}
In recent times, there has been an increased interest in finding complex
relationships underlying biological processes, such as gene expression pathways
or connections between neurons in the brain. In the past, many approaches have
focused on \emph{directed graphical models}, in which nodes are random
variables and the structure of edges forces the joint distribution to factor in
a certain way. Some approaches have instead focused on \emph{undirected
	graphical models}, in which the nodes are also some variables of interest, but
in which the existence of edges imposes a certain conditional independence
structure on the variables. This report develops methods to infer edges in an
undirected graph.

The Bayesian method requires the specification of a prior distribution over the
graphs, which can encode specific domain knowledge. For instance, the estimated
graph is often chosen to be sparse,
i.e.\ to have few edges.

Most inference approaches for undirected Bayesian graphs have focused on
stochastic methods which obtain an estimate of the full posterior distribution
using numerical sampling methods such as Markov chain Monte Carlo (MCMC) with
Gibbs sampling, \parencite{wang-2015}. However, for most practical applications
point estimates are sufficient, so we will follow~\cite{limcco-2017}, who
derive an expectation conditional maximisation (ECM) approach to inference. We
will then extend their method to multiple graphs. \cite{luke2017} have a
``forns"(?) fairly similar to ours, but we will use a probit link to pool
information across the graphs (they use a logistic link), and we will infer the
pairwise similarity between the graphs.

\subsection{Basics of Bayesian statistics}
Let $y_1, \dots, y_n$ random variables whose distribution is $p(y)$. Suppose
the existence of a set of random distribution functions $P(y)$ containing
$p(y)$, and of a set $\Theta$ and a function $\varphi: \Theta \to P(y)$ that we
call a \emph{parametrisation}. Then, statistics is concerned with using the
observed $y_1,\dots,y_n$ to estimate $\theta \in \Theta$ such that
$\varphi(\theta)$ is as ``close" as possible to the ``true" $p(y)$.

Unlike in the usual statistical context, in Bayesian statistics we view the parameter $\theta$ as
being itself random with some distribution $p(\theta)$, called \emph{prior distribution}. We define the joint
probability distribution of $y$ and $\theta$ as being a function $p(y,\theta)$ such that
\[p(\theta)         = \int p(y,\theta) dy      \]
and
\[p(y)  = \int p(y,\theta) d\theta.\]
Then we define the conditional distribution ``of $y$ given $\theta$" as
\[
	p(y \mid \theta) = \frac{p(y,\theta)}{p(\theta)}.
\]
Using this property twice, we obtain Bayes' theorem, which allows us to ``invert"
and obtain the distribution of $\theta$ given the observed $y$:
\[ p(\theta \mid y) = \frac{p(y,\theta)}{p(y)} = \frac{p(\theta)p(y \mid \theta)}{p(y)}. \]

\subsection{Bayesian hierarchical models}
A hierarchical model involves conditional prior distributions over all model parameters
$\theta_1, \dots, \theta_p$. This results in a hierarchical structure, and the
higher a parameter is placed
up the hierarchy, the higher the number of samples we need to have to produce
confident estimates of it.
For example, suppose that we observe values $y_{ij} \iid P(\theta_j)$ where $j=1,\dots,p$ and
$i=1,\dots,n_j$. That is, we have a certain number of observations,
each belonging to some group, and the values within each group have
parameter $\theta_j$. We will now discuss two methods to analyze such data
that will motivate the use of hierarchical models.

It might at first seem appealing to ignore the differences between groups. This
means setting all the $\theta_j$ to be equal to each other, so we can perform
usual maximum likelihood estimation. If there is a group with only a few
outlying observations, the maximum likelihood estimator will have high bias, and it will have an estimate that is far from its observations.

Then another idea might be to treat each group in a separate estimation procedure, assuming
that they are unrelated. In some cases, this is
justified, but the group with only a few
observations will have an estimate with a large variance.

A hierarchical model sees the first two methods as two
extremes: ``the $\theta_j$ are the same" and ``the $\theta_j$ are unrelated".
Would there be a way to automatically decide how similar or how different the $\theta_j$ should be from each other?
Yes, we will imagine that the $\theta_j$ are themselves independent and identically distributed
$\theta_j \mid \phi \sim Q(\phi),$ for some parameter $\phi.$ Then the posterior
joint distribution can be expressed as
\[p(\phi, \theta \mid y) \propto p(y \mid \theta) p(\theta \mid \phi) p(\phi).\]
The distribution $p(\phi)$ chosen for $\phi$, is often
referred to as a \emph{hyperprior}.

\section{Undirected graphical models for multivariate Gaussian
  variables}\label{sect:graphs}
An undirected graphical model represents the conditional independence
structure of some variables of interest $y_1, \dots, y_p$ using a
graph $G=(V, E),$ with $V = \{y_1, \dots, y_p\}$ such that an edge $(y_i, y_j)$
exists in $E$ if and only if $y_i$ and $y_j$ are dependent given all the other
variables (which we denote as $y_{-ij}$, for $i,j \in \{1,\dots,p\}$). So in summary
\[y_i \nindep y_j \mid y_{-ij} \iff (y_i, y_j) \in E.\]
For a sample
of $n$ $p$-dimensional multivariate Gaussian variables $y^1, \dots, y^n \sim
	\Np(0, \Sigma)$ we would like to deduce the structure of the graph $G$ by
estimating the \emph{precision matrix} $\Omega =
	\Sigma\inv$ and by observing that a given entry $\omega_{i,j}$ is zero if and only if $y_i$ and $y_j$ are
independent given $y_{-ij}$, i.e.,
\[(y_i, y_j) \notin E \iff \omega_{i,j} = 0.\]

\subsection{Spike and slab prior for graphical models}
Let $y \in \R^p$ be random vector distributed under the hierarchical model
\begin{align*}
	y \mid \Omega                  & \sim \Np(0, \Omega\inv)\ \Omega \in M^+,                                                                     \\
	\omega_{i,j} \mid \delta_{i,j} & \sim \delta_{i,j} \Nor(0, v_1^2) + (1 - \delta_{i,j}) \Nor(0, v_0^2), \quad i \neq j, \quad v_0^2 \ll v_1^2, \\
	\omega_{i,i}                   & \sim \textsc{Exp}(\lambda / 2),                                                                              \\
	\delta_{i,j} \mid \pi          & \sim \Bern(\pi),                                                                                             \\
	\pi                            & \sim \Beeta(a,b),
\end{align*}
where $M^+$ is the set of symmetric positive definite matrices, $\Np(0, \Omega\inv)$ is the multivariate normal distribution with mean $0$ and
covariance matrix $\Omega\inv$, and $a, b, \lambda, v_0, v_1 \in \R$ are hyperparameters.
The entries $\omega_{i,j}$ are so that the conditional distribution of $\Omega$ as a whole can be written as
\[p(\Omega \mid \delta) = C\inv \prod_{j < k}  \Nor(\omega_{jk} \mid 0,
	v_{\delta_{jk}}^2) \prod_j \Exp\left(\omega_{jj} \mid \frac{\lambda}{2}\right) \1\{\Omega \in M^+ \},\]
with $C$ a constant that depends on $\delta, v_0, v_1, \lambda$.
This is known as the continuous \emph{spike-and-slab} prior because it
corresponds to a mixture of two Gaussian distributions, one with a small variance
$v_0^2$ (the spike) and one with a large variance $v_1^2$ (the slab).
Under this continuous spike-and-slab, if an entry $\omega_{i,j}$ is truly zero,
it is absorbed in the spike and will be estimated as close to zero. The
discrete spike-and-slab instead uses a point mass at zero, $\1\{\omega_{i,j} =
	0\}.$ We use the former because it makes the computation of the posterior
distribution simpler.
Finally, let $Y \in \R^{n \times p}$ be the matrix whose rows are identically and independently distributed
observations of $y$.

Given this, we seek values of $\Omega, \delta, \pi$ that
maximise the log posterior joint distribution $\log p(\Omega, \delta, \pi \mid
	Y)$. The posterior joint distribution can be
decomposed as
\begin{equation}\label{eq:decomp}
	p(\Omega, \delta, \pi \mid Y) = p(\Omega \mid \delta) p(\delta \mid \pi) p(Y \mid
	\Omega) p(\pi) p(Y)\inv.
\end{equation}
The factor $p(Y)\inv$ is constant, and hence has no influence on the
maximisation. Using the definitions and the decomposition of~\eqref{eq:decomp} we find that
$\log p(\Omega, \delta, \pi \mid Y)$ equals
\begin{multline}\label{eq:joint}
	\text{constant} + \sum_{i<j} \left[-\log\left\{v_0^2 (1 - \delta_{ij}) +
	v_1^2 \delta_{ij}\right\} - \frac{\omega^2_{ij}}{2} \frac{1}{v_0^2 (1 -
		\delta_{ij}) + v_1^2 \delta_{ij}} \right] - \sum_i \frac{\lambda}{2}
	\omega_{ii}\\
	+ \sum_{i < j} \left\{\delta_{ij} \log(\pi) + (1 - \delta_{ij})\log(1 - \pi)\right\}                                                                                                                \\
	+ (a-1) \log(\pi) + (b-1)\log(1 - \pi) + \frac{n}{2} \log\det(\Omega) -\frac{1}{2}\tr (Y^t Y \Omega).
\end{multline}

\subsection{Expectation Maximisation for Bayesian graphical models}
Following~\cite{limcco-2017} instead of maximising~\eqref{eq:joint} we
iteratively maximise its expectation over $\delta$. Taking the expectation
of~\eqref{eq:joint} we obtain
\begin{equation}\label{eq:cond}
	Q(\Omega, \pi \mid \Omega\powl, \pi\powl, Y) = \mathbb{E}_{\delta \mid \Omega\powl, \pi\powl, Y}\left\{\log p(\Omega, \delta, \pi| X) \Bigm\vert  \Omega\powl, \pi\powl, Y\right\}.
\end{equation}
Where $\Omega\powl$ and $\pi\powl$ denote the values obtained for $\Omega$ and
$\pi$ at the $l$-th iteration of the algorithm, respectively.
Equation~\eqref{eq:cond} is equal to
\begin{multline}
	\text{constant} - \sum_{i<j} \frac{\omega_{ij}^2}{2} \E_{\delta_{ij} |
		\cdot}\left(\frac{1}{v_0^2 (1 - \delta_{ij}) + v_1^2 \delta_{ij}}\right) -
	\sum_i \frac{\lambda}{2} \omega_{ii}
	\\
	+ \frac{p(p-1)}{2}  \log(1 - \pi) + \sum_{i<j} \E_{\delta_{ij} |
		\cdot}(\delta_{ij}) \log\left(\frac{\pi}{1-\pi}\right)
	\\
	+ (a - 1) \log(\pi) + (b - 1) \log(1 - \pi)
	+ \frac{n}{2} \log\det(\Omega) - \frac{1}{2} \tr(Y^t Y \Omega),
\end{multline}
where $\mathbb{E}_{\delta_{ij} \mid \cdot}$ denotes the conditional expectation with respect
to $\delta \mid \Omega\powl, \pi\powl, Y.$
The expectation terms are
\begin{equation}\label{eq:qij}
	\E_{\delta_{ij} \mid \cdot}(\delta_{ij}) = p\left(\delta_{ij} = 1 \mid \omega_{ij}\powl, \pi\powl\right)
	= \frac{ \pi\powl p\left(\omega_{ij}\powl \mid \delta_{ij} = 1\right)}{\pi\powl p\left(\omega_{ij}\powl \mid \delta_{ij} = 1\right) +
		(1 - \pi\powl) p\left(\omega_{ij}\powl \mid \delta_{ij} = 0\right)}
\end{equation}
which we denote $q_{ij},$ and
\begin{equation}\label{eq:dij}
	d_{ij} := \E_{\delta_{ij} \mid \cdot}\left(\frac{1}{v_0^2 (1 -
			\delta_{ij}) + v_1^2 \delta_{ij}}\right) = \sum_{\delta_{ij} = 0}^1
	\frac{p\left(\delta_{ij} \mid \omega_{ij}\powl, \pi\powl\right)}{v_0^2 (1 - \delta_{ij}) +
		v_1^2 \delta_{ij}} = \frac{q_{ij}}{v_1^2} + \frac{1 - q_{ij}}{v_0^2}.
\end{equation}
This is the \emph{expectation step} (E step).
Now we use~\eqref{eq:qij} and~\eqref{eq:dij} to compute the next iterates $\pi^{(l+1)}$ and $\Omega^{(l+1)}$.
The derivative of~\eqref{eq:cond} with respect to $\pi$ is
\[\pi\left\{\frac{p(p-1)}{2} - a - b + 2\right\} + \sum_{i<j} q_{ij} + a - 1,\]
and it is equal to zero when \[\pi = \frac{a-1 + \sum_{i<j}q_{ij}}{a + b - 2 +
	\frac{p(p-1)}{2}}.\]
The maximisation with respect to $\Omega$ requires that $\Omega$ remains positive definite
after each iteration. In~\cite{wang-2015} the authors show that if we
slice the matrices $\Omega$, $Y^t Y$ and $V = (v_{\delta_{ij}})_{ij}$ in the following way
\[\Omega = \begin{pmatrix}
		\Omega_{11}   & \omega_{12} \\
		\omega_{12}^t & \omega_{22}
	\end{pmatrix},
	\quad
	Y^t Y = \begin{pmatrix}
		S_{11}   & s_{12} \\
		s_{12}^t & s_{22}
	\end{pmatrix},
	\quad
	V = \begin{pmatrix}
		V_{11}   & v_{12} \\
		v_{12}^t & v_{22}
	\end{pmatrix},
\]
where $\omega_{22}$ is a scalar and $\omega_{12}$ is a $(p-1)$-dimensional
vector (likewise for $s_{22}$, $s_{12}$, and $v_{12}, v_{22}$), we find the
conditional distributions
\[
	\omega_{12} \mid \delta, Y \sim \Nor(-C^{-1}s_{12}, C) \quad C=(s_{22} +
	\lambda) \Omega_{11}^{-1} + \diag(v_{12}^{-1}),
\]
and
\[\omega_{22} \mid \delta , Y \sim \textsc{Gamma}\left(\frac{n}{2} + 1, \frac{s_{22} +
		\lambda}{2}\right) + \omega_{12}^t\Omega_{11}^{-1}\omega_{12}.\]
The term $v_{12}^{-1}$ refers to the vector $v_{12}$ after we inverted each
component, so $\E(v_{12}^{-1}) = d_{12}$, where $d_{12}$ is the
vector of $d_{ij}$ values defined similarly as $\omega_{12}$.
Taking the mode of these distributions gives

\begin{align*}
	\omega_{12}^{(l+1)} & = -\left\{(s_{22} + \lambda) \Omega_{11}^{-1} + \diag(d_{12})\right\}^{-1} s_{12}                     \\
	\omega_{22}^{(l+1)} & = \frac{n}{s_{22} + \lambda} + \left(\omega_{12}^{(l+1)}\right)^t \Omega_{11}^{-1}\omega_{12}^{(l+1)}
\end{align*}

\section{Extending to multiple graphs}
We now have the following hierarchical model
\begin{align*}
	y_k \mid \Omega_k              & \sim \Np(0, \Omega_k^{-1}),                                                                                \\
	\omega_{ijk} \mid \delta_{ijk} & \sim \delta_{ijk} \Nor(0, v_1^2) + (1 - \delta_{ijk}) \Nor(0, v_0^2) \quad i \neq j, \quad v_0^2 \ll v_1^2 \\
	\omega_{iik}                   & \sim \Exp(\lambda_k/2),                                                                                    \\
	\delta_{ijk} \mid \theta_{ijk} & \sim \Bern(\Phi(\theta_{ijk})),                                                                            \\
	\theta_{ij}                    & \sim \textsc{N}_K(0, \Sigma),
\end{align*}
where $\Phi$ is the standard normal cumulative distribution function, $v_0,
	v_1, \lambda_k, \Sigma$ are hyperparameters, and $k=1,\dots,K$ where $K$ is the
number of graphs.

The log posterior joint distribution, $\log p(\Omega_k, \theta_k, \delta_k \mid y_k)$ is
\begin{multline}\label{eq:mg-joint}
	\text{constant} + \frac{1}{2} \log\det(\Omega_k) - \frac{1}{2} y_k \Omega_k
	y_k^t + \sum_{i < j} -\log(v_{\delta_{ijk}}) - \frac{\omega_{ijk}^2}{2
		v_{\delta_{ijk}}^2} - \frac{\lambda_k}{2} \tr(\Omega_k) \\ + \frac{p(p-1)}{2}
	\log\left\{1 - \Phi(\theta_{ijk})\right\} + \sum_{i < j} \delta_{ijk} \log
	\left\{\frac{\Phi(\theta_{ijk})}{1 - \Phi(\theta_{ijk})}\right\} + \log p(\theta_k)
\end{multline}

Taking the conditional expectation over $\delta$ of \eqref{eq:mg-joint} gives
\begin{multline}
	\E_{\delta_{ijk} \mid \Omega_k\powl, \theta_k\powl, y_k} \left\{\log p(\Omega_k, \theta_k, \delta_k \mid y_k ) \mid \Omega_k\powl, \theta_k\powl, y_k  \right\} = \\
	\text{constant} + \frac{1}{2} \log\det(\Omega_k) - \frac{1}{2} y_k \Omega_k
	y_k^t + \sum_{i < j} - \frac{\omega_{ijk}^2}{2}\E_{\delta_{ijk} \mid \cdot}\left(\frac{1}{\delta_{ijk} v_1^2 + (1 - \delta_{ijk}) v_0^2}\right) - \frac{\lambda_k}{2} \tr(\Omega_k) \\ + \frac{p(p-1)}{2}
	\log\left\{1 - \Phi(\theta_{ijk})\right\} + \sum_{i < j} \E_{\delta_{ijk} \mid \cdot}(\delta_{ijk}) \log
	\left\{\frac{\Phi(\theta_{ijk})}{1 - \Phi(\theta_{ijk})}\right\} + \sum_{i < j} \theta_{ij}^t \Sigma\inv \theta_{ij},
\end{multline}
where $\E_{\delta_{ijk} \mid \cdot}$ refers to the expectation over
$\delta_{ijk} \mid \Omega_k\powl, \theta_k\powl, y_k.$ The new expectation
terms are computed in a similar fashion as before
\[
	q_{ijk} := \E_{\delta_{ijk} \mid \cdot}(\delta_{ijk})
	= \frac{\Phi(\theta_{ijk}) \Nor\left(\omega_{ijk}; 0, v_1^2\right)}{\Phi(\theta_{ijk})
		\Nor\left(\omega_{ijk}; 0, v_1^2\right) + \{1 - \Phi(\theta_{ijk}) \}
		\Nor\left(\omega_{ijk}; 0, v_0^2\right)},
\]
and
\[
	d_{ijk} := \E_{\delta_{ijk} \mid \cdot} \left(\frac{1}{\delta_{ijk} v_1^2 +
			(1 - \delta_{ijk}) v_0^2}\right) = \frac{q_{ijk}}{v_1^2} + \frac{(1 -
		q_{ijk})}{v_0^2}
\]

\section{Choosing hyperparameters}
The performance of the model is heavily influenced by the values of the
hyperparameters. On Figure~\ref{fig:mean_auc} we have plotted how the area
under curve (AUC) changes for different values of $v_0.$ In the single-graph
case we decide to to fix $v_1 = 100,
	\lambda = 2,$ and $a = 1$. Then we count the number of off-diagonal entries in
the empirical precision matrix $\bar \Omega$ that are outside the interval
$[-t, t],$ for $t \in \R$. This gives a rough estimate of the expected number
of edges, $s$. We choose $b$ so that the mean of the Beta distribution,
$a/(a+b),$ is equal to the proportion of expected edges $2s/\{p(p-1)\}.$ We then
run the ECM algorithm with a different choice of $v_0$ each time. We pick the
value of $v_0$ for which the output $\Omega$ has proportion of edges closest to
our estimated proportion.

As we can see, the choice of $t$ is critical as it will decide the sparsity level.
To choose $t$ there are two approaches. First, we can ourselves choose the
expected sparsity level and pick $t$ accordingly. Otherwise we can look at a
histogram of the off-diagonal values of $\bar \Omega$ and choose a value of $t$
where the spike and the slab seem to ``cross" each other.

\begin{figure}
	\label{fig:mean_auc}
	\centering
	\includegraphics[width=.7\textwidth]{pictures/Rplot01.png}
	\caption{Mean AUC over ten replicates for different values of $v_0$, with a
		graph with 25 nodes and 100 samples.}
\end{figure}

\section{Numerical techniques to make ECM more stable}
todo!

\section{Simulations}
We performed a set of simulations with data generated from the R package
\texttt{huge}~\parencite{huge2020}, we compare the results of our method with that of~\cite{mein2006} which for a given node $i$, computes
\[\hat{\theta}^{i, \lambda} = \argmin\displaylimits_{\theta : \theta_i = 0}\frac{1}{n} ||Y_i - Y \theta||_2^2 + \eta ||\theta||_1,\]
where $Y_i$ is the $i$-th column of $Y$, $\theta_j^i = -\omega_{ij}/\omega_{ii}$, and $\eta$ is a constant that controls the $l_1$ penalty term.
We only use one replicate but plan to add more, so we can obtain measures of uncertainty for the performance of our method.
The tests were performed on different graph structures that \texttt{huge} allows us to generate.
In Table~\ref{tab:cheat} and \ref{tab:honest}, the lines labelled by ``random" indicate that the
underlying graph was generated such that each edge had an equal probability. For the lines labelled by ``cluster" instead, vertices were split
into groups and an edge had a higher probability of appearing when the vertices
belonged in the same group.
\begin{table}
	\centering
	\small
	\caption{$n=50$, $p \in \{25, 50, 100\}$, F1 scores. For EMGS, hyperparameters were selected by maximising F1 score.}\label{tab:cheat}
	\begin{tabular}{|l||c|c|c|c|}
		\hline
		graph                    & method & 25   & 50   & 100  \\
		\hline
		\multirow{2}{*}{random}  & EMGS   & 0.87 & 0.83 & 0.75 \\
		                         & mb     & 0.89 & 0.85 & 0.76 \\
		\hline
		\multirow{2}{*}{cluster} & EMGS   & 0.73 & 0.70 & 0.67 \\
		                         & mb     & 0.75 & 0.70 & 0.63 \\
		\hline
	\end{tabular}
\end{table}
\begin{table}
	\centering
	\small
	\caption{$n=200$, $p \in \{25, 35, 50\}$. F1 scores. For EMGS, hyperparameters were selected by maximising the posterior joint distribution.}\label{tab:honest}
	\begin{tabular}{|l||c|c|c|c|}
		\hline
		graph                    & method & 25   & 50   & 100  \\
		\hline
		\multirow{2}{*}{cluster} & EMGS   & 0.68 & 0.67 & 0.67 \\
		                         & mb     & 0.89 & 0.87 & 0.89 \\
		\hline
	\end{tabular}
\end{table}
Our method, EMGS, estimates $\Omega$ and $\pi$. We use~\eqref{eq:qij} with the estimates to obtain the posterior probabilities of inclusion of all edges $(y_i, y_j)$. The method by~\cite{mein2006} estimates $\Omega$ as a function of $\eta.$ Higher $\eta$ values increase the number of zero entries in their estimate of $\Omega.$
We compare maximal \emph{F1 scores} which is the harmonic mean between \emph{precision}
and \emph{recall}, where the precision is the fraction of true edges
among the edges that the method detects, and recall is the fraction
of true edges which the method detects. These terms are also known as the
positive predictive value and true positive rate.
We fixed $a = b = \lambda = 1, v_1 = 100$ and tried varying
values of $v_0$ between $1e-4$ and $1e-3$.
A better approach will need to be developed in the future.
The choice of $v_0$ and $v_1$ has an impact on the performance of the algorithm.
Indeed, in Table~\ref{tab:cheat} we selected the ``best" $v_0$ using the ground truth.
Table~\ref{tab:honest} provides a more ``honest" approach in which we picked $v_0$ by
maximising the posterior distribution.
However, this often produced degenerate selections with the posterior probabilities of inclusion $p(\delta_{ij} \mid \Omega, \pi, Y)$ collapsing to either zero or one.

Once we have derived the ECM algorithm for multiple graphs we will also do a variety of simulations in that setting.
Also, we will display ROC curves, and other interesting plots once we find a good ``honest" way to select the hyperparameters.
The code for the simulations can be found on \href{https://github.com/jkasalt/pdm_summary}{\texttt{https://github.com/jkasalt/pdm\_summary}}.

\section{Application to gene-related dataset}
todo!

\section{Conclusion}
todo!

\printbibliography
\end{document}

