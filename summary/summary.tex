\documentclass[a4paper, 11pt, oneside]{report}
\usepackage[MScThesis]{EPFLreport}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{booktabs}

\usepackage{tikz}

\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\cof}{cof}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\argmin}{argmin}
\newcommand{\iid}{\stackrel{\textmd{i.i.d.}}{\sim}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\1}{\mathds{1}}
\newcommand{\powl}{^{(l)}}
\newcommand{\inv}{^{-1}}
\newcommand{\Np}{\textsc{N}_p}
\newcommand{\Nor}{\textsc{N}}
\newcommand{\sns}{\textsc{sns}}
\newcommand{\Bern}{\textsc{Bern}}
\newcommand{\Beeta}{\textsc{Beta}}
\newcommand{\Exp}{\textsc{Exp}}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\nindep}{{\rotatebox[origin=c]{90}{$\nvDash$}}}
\newcommand{\bOmega}{\mathbf{\Omega}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\btheta}{\mathbf{\theta}}
\newcommand{\tijkl}{\theta_{ijk}\powl}
\newcommand{\bY}{\mathbf{Y}}

\author{Luca Bracone}
\title{Bayesian joint inference of multiple graphical models using
	spike-and-slab priors}

\begin{document}
\maketitle
% \makededication
% \makeacks

\begin{abstract}
	Graphical models are models in which the conditional independence of
	variables is represented as a graph. If the variables are assumed to be
	distributed following a multivariate Gaussian distribution, the conditional
	independence is given by the zero entries of the precision matrix. Using a
	spike and slab prior we aim to clearly differentiate between the zero and
	non-zero entries. Rather than using Gibbs-sampling methods, our work makes
	use of an expectation conditional maximisation algorithm (ECM) in order to
	obtain fast pointwise estimates. We extend previously done work by focusing
	on the analysis of multiple graphs. In doing so we leverage shared
	information about graphs to obtain better estimates. We show on simulated
	data that our method produces better estimates than other single-graph
	methods.
\end{abstract}

\chapter{Introduction}
\section{Motivation}
In recent times, there has been an increased interest in finding complex
relationships underlying biological processes, such as gene expression pathways
or connections between neurons in the brain. In the past, many approaches have
focused on \emph{directed graphical models}, in which nodes are random
variables and the structure of edges forces the joint distribution to factor in
a certain way. Some approaches have instead focused on \emph{undirected
	graphical models}, in which the nodes are also some variables of interest, but
in which the existence of edges imposes a certain conditional independence
structure on the variables. This report develops methods to infer edges in an
undirected graph.

We use a Bayesian framework. It allows us to specify a prior distribution over the
graphs, which can encode specific domain knowledge. For instance, the estimated
graph is often chosen to be sparse, i.e.\ to have few edges.
There are many possible priors one can choose from.
The conjugate $G$-Wishart prior derived by~\cite{HyperInverseWRovera2002}
has usually been a common choice.
Recently, other priors have started being used because they proved to have
better computational scalability as the number of parameters increases.
Those include graphical horseshoe~\parencite{cami2022}, the spike-and-slab
graphical lasso~\parencite{limm2018}, and the
spike-and-slab~\parencite{wang-2015} which is the one we will use.

Most inference approaches for undirected Bayesian graphs have focused on
stochastic methods which obtain an estimate of the full posterior distribution
using numerical sampling methods such as Markov chain Monte Carlo (MCMC) with
Gibbs sampling, \parencite{wang-2015}. However, for most practical applications
point estimates are sufficient, so we will follow~\cite{limcco-2017}, who
derive an expectation conditional maximisation (ECM) approach to inference. We
will then extend their method to multiple graphs.~\cite{luke2017} have a method that is
fairly similar to ours, but we will use a probit link to pool
information across the graphs (they use a logistic link), and we will infer the
pairwise similarity between the graphs.

\section{Bayesian hierarchical models}
A hierarchical model involves conditional prior distributions over all model parameters
$\theta_1, \dots, \theta_p$. This results in a hierarchical structure, and the
higher a parameter is placed
up the hierarchy, the higher the number of samples we need to have to produce
confident estimates of it.
For example, suppose that we observe values $y_{ij} \iid P(\theta_j)$ where $j=1,\dots,p$ and
$i=1,\dots,n_j$. That is, we have a certain number of observations,
each belonging to some group, and the values within each group have
parameter $\theta_j$. We will now discuss two methods to analyze such data
that will motivate the use of hierarchical models.

It might at first seem appealing to ignore the differences between groups. This
means setting all the $\theta_j$ to be equal to each other, so we can perform
usual maximum likelihood estimation. If there is a group with only a few
outlying observations, the maximum likelihood estimator will have high bias, and it will have an estimate that is far from its observations.

Then another idea might be to treat each group in a separate estimation procedure, assuming
that they are unrelated. In some cases, this is
justified, but the group with only a few
observations will have an estimate with a large variance.

A hierarchical model sees the first two methods as two
extremes: ``the $\theta_j$ are the same'' and ``the $\theta_j$ are unrelated''.
Would there be a way to automatically decide how similar or how different the $\theta_j$ should be from each other?
Yes, we will imagine that the $\theta_j$ are themselves independent and identically distributed
$\theta_j \mid \phi \sim Q(\phi)$, for some parameter $\phi$. Then the posterior
joint distribution can be expressed as
\[p(\phi, \theta \mid y) \propto p(y \mid \theta) p(\theta \mid \phi) p(\phi).\]
The distribution $p(\phi)$ chosen for $\phi$, is often
referred to as a \emph{hyperprior}.

\chapter{Undirected graphical models for multivariate Gaussian
  variables}\label{sect:graphs}
An undirected graphical model represents the conditional
structure of some variables of interest $y_1, \dots, y_p$ using a
graph $G=(V, E)$, with $V = \{y_1, \dots, y_p\}$ such that an edge $(y_i, y_j)$
exists in $E$ if and only if $y_i$ and $y_j$ are dependent given all the other
variables (which we denote as $y_{-ij}$, for $i,j \in \{1,\dots,p\}$). So in summary
\[y_i \nindep y_j \mid y_{-ij} \iff (y_i, y_j) \in E.\]
For a sample
of $n$ $p$-dimensional multivariate Gaussian variables $y^1, \dots, y^n \sim
	\Np(0, \Sigma)$ we would like to deduce the structure of the graph $G$ by
estimating the \emph{precision matrix} $\Omega =
	\Sigma\inv$ and by observing that a given entry $\omega_{i,j}$ is zero if and only if $y_i$ and $y_j$ are
independent given $y_{-ij}$, i.e.,
\[(y_i, y_j) \notin E \iff \omega_{i,j} = 0.\]

\section{Spike and slab prior for graphical models}
Let $y \in \R^p$ be random vector distributed under the hierarchical model
\begin{align*}
	y \mid \Omega                  & \sim \Np(0, \Omega\inv)\ \Omega \in M^+,                                                                     \\
	\omega_{i,j} \mid \delta_{i,j} & \sim \delta_{i,j} \Nor(0, v_1^2) + (1 - \delta_{i,j}) \Nor(0, v_0^2), \quad i \neq j, \quad v_0^2 \ll v_1^2, \\
	\omega_{i,i}                   & \sim \textsc{Exp}(\lambda / 2),                                                                              \\
	\delta_{i,j} \mid \pi          & \sim \Bern(\pi),                                                                                             \\
	\pi                            & \sim \Beeta(a,b),
\end{align*}
where $M^+$ is the set of symmetric positive definite matrices, $\Np(0, \Omega\inv)$ is the multivariate normal distribution with mean $0$ and
covariance matrix $\Omega\inv$, and $a, b, \lambda, v_0, v_1 \in \R$ are hyperparameters.
The entries $\omega_{i,j}$ are so that the conditional distribution of $\Omega$ as a whole can be written as
\[p(\Omega \mid \delta) = C\inv \prod_{j < k}  \Nor(\omega_{jk} \mid 0,
	v_{\delta_{jk}}^2) \prod_j \Exp\left(\omega_{jj} \mid \frac{\lambda}{2}\right) \1\{\Omega \in M^+ \},\]
with $C$ a constant that depends on $\delta, v_0, v_1, \lambda$.
This is known as the continuous \emph{spike-and-slab} prior because it
corresponds to a mixture of two Gaussian distributions, one with a small variance
$v_0^2$ (the spike) and one with a large variance $v_1^2$ (the slab).
Under this continuous spike-and-slab, if an entry $\omega_{i,j}$ is truly zero,
it is absorbed in the spike and will be estimated as close to zero. The
discrete spike-and-slab instead uses a point mass at zero, $\1\{\omega_{i,j} =
	0\}$. We use the former because it makes the computation of the posterior
distribution simpler.
Finally, let $Y \in \R^{n \times p}$ be the matrix whose rows are identically and independently distributed
observations of $y$.

Given this, we seek values of $\Omega, \delta, \pi$ that
maximise the log posterior joint distribution $\log p(\Omega, \delta, \pi \mid
	Y)$. The posterior joint distribution can be
decomposed as
\begin{equation}\label{eq:decomp}
	p(\Omega, \delta, \pi \mid Y) = p(\Omega \mid \delta) p(\delta \mid \pi) p(Y \mid
	\Omega) p(\pi) p(Y)\inv.
\end{equation}
The factor $p(Y)\inv$ is constant, and hence has no influence on the
maximisation. Using the definitions and the decomposition of~\eqref{eq:decomp} we find that
$\log p(\Omega, \delta, \pi \mid Y)$ equals
\begin{multline}\label{eq:joint}
	\text{constant} + \sum_{i<j} \left[-\log\left\{v_0^2 (1 - \delta_{ij}) +
	v_1^2 \delta_{ij}\right\} - \frac{\omega^2_{ij}}{2} \frac{1}{v_0^2 (1 -
		\delta_{ij}) + v_1^2 \delta_{ij}} \right] - \sum_i \frac{\lambda}{2}
	\omega_{ii}\\
	+ \sum_{i < j} \left\{\delta_{ij} \log(\pi) + (1 - \delta_{ij})\log(1 - \pi)\right\}                                                                                                                \\
	+ (a-1) \log(\pi) + (b-1)\log(1 - \pi) + \frac{n}{2} \log\det(\Omega) -\frac{1}{2}\tr (Y^t Y \Omega).
\end{multline}

\section{Expectation Maximisation for Bayesian graphical models}
Following~\cite{limcco-2017} instead of maximising~\eqref{eq:joint} we
iteratively maximise its expectation over $\delta$. Taking the expectation
of~\eqref{eq:joint} we obtain
\begin{equation}\label{eq:cond}
	Q(\Omega, \pi \mid \Omega\powl, \pi\powl, Y) = \mathbb{E}_{\delta \mid
		\Omega\powl, \pi\powl, Y}\left\{\log p(\Omega, \delta, \pi| X) \Bigm\vert
	\Omega\powl, \pi\powl, Y\right\}.
\end{equation}
Where $\Omega\powl$ and $\pi\powl$ denote the values obtained for $\Omega$ and
$\pi$ at the $l$-th iteration of the algorithm, respectively.
Equation~\eqref{eq:cond} is equal to
\begin{multline}
	\text{constant} - \sum_{i<j} \frac{\omega_{ij}^2}{2} \E_{\delta_{ij} |
		\cdot}\left(\frac{1}{v_0^2 (1 - \delta_{ij}) + v_1^2 \delta_{ij}}\right) -
	\sum_i \frac{\lambda}{2} \omega_{ii}
	\\
	+ \frac{p(p-1)}{2}  \log(1 - \pi) + \sum_{i<j} \E_{\delta_{ij} |
		\cdot}(\delta_{ij}) \log\left(\frac{\pi}{1-\pi}\right)
	\\
	+ (a - 1) \log(\pi) + (b - 1) \log(1 - \pi)
	+ \frac{n}{2} \log\det(\Omega) - \frac{1}{2} \tr(Y^t Y \Omega),
\end{multline}
where $\mathbb{E}_{\delta_{ij} \mid \cdot}$ denotes the conditional expectation with respect
to $\delta \mid \Omega\powl, \pi\powl, Y$.
The expectation terms are
\begin{equation}\label{eq:qij}
	\E_{\delta_{ij} \mid \cdot}(\delta_{ij}) = p\left(\delta_{ij} = 1 \mid \omega_{ij}\powl, \pi\powl\right)
	= \frac{ \pi\powl p\left(\omega_{ij}\powl \mid \delta_{ij} = 1\right)}{\pi\powl p\left(\omega_{ij}\powl \mid \delta_{ij} = 1\right) +
		(1 - \pi\powl) p\left(\omega_{ij}\powl \mid \delta_{ij} = 0\right)}
\end{equation}
which we denote $q_{ij},$ and
\begin{equation}\label{eq:dij}
	d_{ij} := \E_{\delta_{ij} \mid \cdot}\left(\frac{1}{v_0^2 (1 -
			\delta_{ij}) + v_1^2 \delta_{ij}}\right) = \sum_{\delta_{ij} = 0}^1
	\frac{p\left(\delta_{ij} \mid \omega_{ij}\powl, \pi\powl\right)}{v_0^2 (1 - \delta_{ij}) +
		v_1^2 \delta_{ij}} = \frac{q_{ij}}{v_1^2} + \frac{1 - q_{ij}}{v_0^2}.
\end{equation}
This is the \emph{expectation step} (E step).
Now we use~\eqref{eq:qij} and~\eqref{eq:dij} to compute the next iterates $\pi^{(l+1)}$ and $\Omega^{(l+1)}$.
The derivative of~\eqref{eq:cond} with respect to $\pi$ is
\[\pi\left\{\frac{p(p-1)}{2} - a - b + 2\right\} + \sum_{i<j} q_{ij} + a - 1,\]
and it is equal to zero when \[\pi = \frac{a-1 + \sum_{i<j}q_{ij}}{a + b - 2 +
	\frac{p(p-1)}{2}}.\]
The maximisation with respect to $\Omega$ requires that $\Omega$ remains positive definite
after each iteration. In~\cite{wang-2015} the authors show that if we
slice the matrices $\Omega$, $Y^t Y$ and $V = {(v_{\delta_{ij}})}_{ij}$ in the following way
\[\Omega = \begin{pmatrix}
		\Omega_{11}   & \omega_{12} \\
		\omega_{12}^t & \omega_{22}
	\end{pmatrix},
	\quad
	Y^t Y = \begin{pmatrix}
		S_{11}   & s_{12} \\
		s_{12}^t & s_{22}
	\end{pmatrix},
	\quad
	V = \begin{pmatrix}
		V_{11}   & v_{12} \\
		v_{12}^t & v_{22}
	\end{pmatrix},
\]
where $\omega_{22}$ is a scalar and $\omega_{12}$ is a $(p-1)$-dimensional
vector (likewise for $s_{22}$, $s_{12}$, and $v_{12}, v_{22}$), we find the
conditional distributions
\[
	\omega_{12} \mid \delta, Y \sim \Nor(-C^{-1}s_{12}, C) \quad C=(s_{22} +
	\lambda) \Omega_{11}^{-1} + \diag(v_{12}^{-1}),
\]
and
\[\omega_{22} \mid \delta , Y \sim \textsc{Gamma}\left(\frac{n}{2} + 1, \frac{s_{22} +
		\lambda}{2}\right) + \omega_{12}^t\Omega_{11}^{-1}\omega_{12}.\]
The term $v_{12}^{-1}$ refers to the vector $v_{12}$ after we inverted each
component, so $\E(v_{12}^{-1}) = d_{12}$, where $d_{12}$ is the
vector of $d_{ij}$ values defined similarly as $\omega_{12}$.
Taking the mode of these distributions gives

\begin{align*}
	\omega_{12}^{(l+1)} & = -{\left\{(s_{22} + \lambda) \Omega_{11}^{-1} + \diag(d_{12})\right\}^{-1}} s_{12}                   \\
	\omega_{22}^{(l+1)} & = \frac{n}{s_{22} + \lambda} + \left(\omega_{12}^{(l+1)}\right)^t \Omega_{11}^{-1}\omega_{12}^{(l+1)}
\end{align*}

\chapter{Extending to multiple graphs}\label{chap:multigraph}
We now have the following hierarchical model
\begin{align*}
	y_k \mid \Omega_k              & \sim \Np(0, \Omega_k^{-1}), \quad k=1,\dots,K                                                                     \\
	\omega_{ijk} \mid \delta_{ijk} & \sim \delta_{ijk} \Nor(0, v_1^2) + (1 - \delta_{ijk}) \Nor(0, v_0^2) \quad i,j = 1,\dots,p, \quad v_0^2 \ll v_1^2 \\
	\omega_{iik}                   & \sim \Exp(\lambda_k/2),                                                                                           \\
	\delta_{ijk}                   & = \1\{z_{ijk} > 0\}                                                                                               \\
	z_{ijk} \mid \theta_{ijk}      & \sim \Nor(\theta_{ijk}, 1),                                                                                       \\
	\theta_{ij}                    & \sim \textsc{N}_K(0, \Sigma),\stepcounter{equation}\tag{\theequation}\label{eq:mg-model}
\end{align*}
where $\Phi$ is the standard normal cumulative distribution function, $v_0,
	v_1, \lambda_k, \Sigma$ are hyperparameters.
Let $Y_k$ be the matrix whose rows are observations of $y_k$, and $\mathbf{Y}$
be $\{Y_1,\dots,Y_K\}$.
The posterior joint distribution $p(\mathbf{\Omega},  \mathbf{z},
	\mathbf{\theta} \mid \mathbf{Y})$ decomposes as
\begin{align}\label{eq:decomp-join}
	p(\bOmega,  \bz, \btheta \mid \bY) & = p(\bY \mid \bOmega) p(\bOmega \mid \bz)
	p(\bz \mid \btheta) p(\btheta) p(\bY)\inv \nonumber                            \\
	                                   & = \prod_{k=1}^K p(Y_k \mid \Omega_k)
	\prod_{i < j}\prod_{k=1}^K p(\omega_{ijk}
	\mid z_{ijk}) \prod_{i < j}\prod_{k=1}^K
	p(z_{ijk} \mid \theta_{ijk})\prod_{i <
		j} p(\theta_{ij}) p(\bY)\inv.
\end{align}
When we take the log of~\ref{eq:decomp-join} and unravel the formula we obtain
\begin{multline}\label{eq:mg-joint}
	- \frac{Kp(p-1)}{2} \log(2\pi) - \frac{p(p-1)}{4} \log\det(\Sigma) + \sum_{k=1}^K \frac{n_k}{2} \log\det(\Omega_k) - \frac{p n_k}{2} \log(2 \pi) -\frac{1}{2} \tr(S_k \Omega_k) \\
	+ \sum_{i < j}\sum_{k=1}^K -\frac{1}{2} \log(2 \pi v_{\delta_{ijk}}) - \frac{\omega_{ijk}^2}{2 v_{\delta_{ijk}}^2} + \sum_{k=1}^K p \log\left(\frac{\lambda_k}{2}\right) - \frac{\lambda_k}{2} \tr(\Omega_k)  \\
	-\frac{1}{2} \sum_{i < j}\sum_{k=1}^K (z_{ijk} - \theta_{ijk})^2  -\frac{1}{2} \sum_{i < j} \theta_{ij}^t \Sigma\inv \theta_{ij} - \log p(\bY)
	% \stepcounter{equation}\tag{\theequation}\label{eq:mg-joint}
\end{multline}
where $S_k = Y_k^t Y_k$.
When we take the conditional expectation of~\eqref{eq:mg-joint} over the latent
variable, $z_k$, we obtain
\begin{multline}\label{eq:mg-exp}
	Q(\mathbf{\Omega}, \mathbf{\theta}) =  \E_{\mathbf{z} \mid
	\mathbf{\Omega}\powl, \mathbf{\theta}\powl, \mathbf{Y}}(\log
	p(\mathbf{\Omega},  \mathbf{z}, \mathbf{\theta} \mid \mathbf{Y}) \mid
	\mathbf{\Omega}\powl, \mathbf{\theta}\powl, \mathbf{Y})                    \\
	= \sum_{k=1}^K \frac{n_k}{2} \log \det(\Omega_k) - \frac{1}{2} \tr(S_k \Omega_k) - \frac{1}{2} \sum_{i < j} \sum_{k=1}^K \omega_{ijk}^2 \E_{\cdot \mid \cdot} \left\{\frac{1}{\1(z_{ijk} > 0) v_1^2 + \1(z_{ijk} \leq 0) v_0^2}\right\} \\
	- \frac{1}{2} \sum_{k=1}^K \lambda_k \tr(\Omega_k)  -\frac{1}{2} \sum_{i < j} \sum_{k = 1}^K \theta_{ijk}^2 - 2 \theta_{ijk} \E_{\cdot \mid \cdot}(z_{ijk}) - \frac{1}{2} \sum_{i < j} \theta_{ij}^t \Sigma\inv \theta_{ij} + \text{constant}
\end{multline}
where $\E_{\cdot \mid \cdot}$ refers to the expectation of $\bz$ conditioned on
$\bOmega\powl, \btheta\powl$, and $\bY$.
We now proceed to the computation of the expectation terms.
First we note that $p(\delta_{ijk} = 1 \mid \bY, \bOmega\powl, \btheta\powl)$
is equal to
\begin{align*}
	       & \frac{p\left(\omega_{ijk}\powl \mid \delta_{ijk}=1\right) p\left(\delta_{ijk}=1 \mid
		\theta_{ijk}\powl\right)}{p\left(\omega_{ijk}\powl \mid \delta_{ijk}=0\right) p\left(\delta_{ijk}=0 \mid
		\theta_{ijk}\powl\right) + p\left(\omega_{ijk}\powl \mid \delta_{ijk}=1\right) p\left(\delta_{ijk}=1 \mid
	\theta_{ijk}\powl\right)}                                                                     \\
	=\quad & \frac{\Nor\left(\omega_{ijk}\powl \mid 0,
		v_1^2\right)\Phi\left(\theta_{ijk}\right)}{\Nor\left(\omega_{ijk}\powl \mid 0,
		v_0^2\right)\{1 -
		\Phi\left(\theta_{ijk}\right)\} +
		\Nor\left(\omega_{ijk}\powl \mid 0, v_1^2\right)
		\Phi\left(\theta_{ijk}\right)}
\end{align*}
Then the first expectation term is given by
\begin{align*}
	 & \E_{\cdot \mid \cdot} \left\{ \frac{1}{v_0^2 \1(z_{ijk} \leq 0) + v_1^2 \1(z_{ijk} > 0)} \right\} \\
	 & = \E_{\cdot \mid \cdot} \left\{ \frac{1}{v_0^2 ( 1 - \delta_{ijk}) + v_1^2 \delta_{ijk}} \right\} \\
	 & = \frac{p(\delta_{ijk} = 1 \mid \bY, \bOmega\powl, \btheta\powl)}{v_1^2}
	+ \frac{1 - p(\delta_{ijk} = 1 \mid \bY, \bOmega\powl,
		\btheta\powl)}{v_0^2}.
\end{align*}
To calculate the other expectation term, $\E_{\cdot \mid \cdot}(z_{ijk})$, we
first see that
\begin{align*}
	p(z_{ijk} \mid & \bY, \bOmega\powl, \delta_{ijk}, \btheta\powl) =
	\frac{p(z_{ijk}, \bY, \bOmega\powl, \delta_{ijk}, \btheta\powl)}{p(\bY, \bOmega\powl, \delta_{ijk}, \btheta\powl)}                                                                                                                                                                                                                                                                                    \\
	               & = \frac{p(\bY \mid \bOmega\powl, z_{ijk}, \delta_{ijk}, \btheta\powl) p(\bOmega\powl \mid z_{ijk}, \delta_{ijk}, \btheta\powl) p(z_{ijk} \mid \delta_{ijk}, \btheta\powl) p(\delta_{ijk} \mid \btheta\powl) p(\btheta\powl)}{p(\bY \mid \bOmega\powl, \delta_{ijk}, \btheta\powl) p(\bOmega\powl \mid \delta_{ijk}, \btheta\powl) p(\delta_{ijk} \mid \btheta\powl) p(\btheta\powl)} \\
	               & = \frac{p(\bY \mid \bOmega\powl) p(\bOmega\powl \mid z_{ijk}) p(z_{ijk} \mid \delta_{ijk}) p(\delta_{ijk} \mid \btheta\powl) p(\btheta\powl)}{p(\bY \mid \bOmega\powl) p(\bOmega\powl \mid \delta_{ijk}) p(\delta_{ijk} \mid \btheta\powl) p(\btheta\powl)}                                                                                                                          \\
	               & = p(z_{ijk} \mid \delta_{ijk}).
	% \stepcounter{equation}\tag{\theequation}\label{eq:cond-z}
\end{align*}
If $\delta_{ijk} = 1$ then $z_{ijk} \mid \delta_{ijk}$ is the same as $z_{ijk}
	\mid z_{ijk} > 0$ which is a truncated normal random variable with mean
\[\theta_{ijk}\powl + \frac{\phi(\theta_{ijk}\powl)}{\Phi(\theta_{ijk}\powl)},\]
where $\phi$ denotes the PDF of a standard normal random variable.
On the other hand, if $\delta_{ijk} = 0$ then $z_{ijk} \mid \delta_{ijk}$ is the
same as $z_{ijk} \mid z_{ijk} \leq 0$ which is also a truncated normal random
variable with mean
\[
	\theta_{ijk}\powl - \frac{\phi(\theta_{ijk}\powl)}{1 - \Phi(\theta_{ijk}\powl)}.
\]
Therefore $\E_{\cdot \mid \cdot}(z_{ijk})$ is equal to
\begin{align*}
	 & \sum_{\delta_{ijk} = 0}^1 \E_{\cdot \mid
		\cdot}\left(z_{ijk} \mid \delta_{ijk}, y_k, \Omega_k, \theta_{ijk}\right)
	p(\delta_{ijk} \mid y_k, \Omega_k, \theta_{ijk})                               \\
	 & = \left\{\tijkl +
	\frac{\phi(\tijkl)}{\Phi(\tijkl)}\right\}
	p(\delta_{ijk} = 1 \mid -) + \left\{\theta_{ijk}\powl -
	\frac{\phi(\theta_{ijk}\powl)}{1 -
	\Phi(\theta_{ijk}\powl)}\right\}p(\delta_{ijk} = 0 \mid -) \label{eq:expect-z} \\
	 & = \tijkl - \frac{\phi(\tijkl)}{1 - \Phi(\tijkl)} + p(\delta_{ijk} = 1
	\mid -) \left\{\frac{\phi(\tijkl)}{\Phi(\tijkl)} + \frac{\phi(\tijkl)}{1
	- \Phi(\tijkl)}\right\}                                                        \\
	 & = \tijkl + M\left(\tijkl, 0\right) + p(\delta_{ijk} = 1 \mid
	-)\left\{M\left(\tijkl, 1\right) -
	M\left(\tijkl, 0\right)\right\},
\end{align*}
where $M(\alpha, c)$ denotes Mill's ratio
\[
	M(\alpha, c) = (-1)^{1-c}\frac{\phi(\alpha)}{\Phi(\alpha)^c \{1 - \Phi(\alpha)\}^c}.
\]
This is the E-step for the multiple graphs setting.
Now for the M-step, we denote $\Xi = \Sigma\inv$ and $\xi_{ij}$ the $(i, j)$-th entry of $\Xi$.
We differentiate $Q(\bOmega, \btheta)$ with respect to
$\theta_{ijk}$ to obtain
\begin{equation}\label{eq:dQdtheta}
	q_{ijk} - \theta_{ijk}  - \xi_{kk} \theta_{ijk} - \sum_{\substack
		{k' = 1 \\ k' \neq k}}^K \xi_{k k'} \theta_{ijk'}.
\end{equation}
Equation~\eqref{eq:dQdtheta} is equal to
zero when
\begin{align*}
	\theta_{ijk} & = \frac{q_{ijk} - \sum_{\substack{k'=1 \\ k' \neq k}}^K \xi_{k k'}\theta_{ijk'}}{ 1 + \xi_{kk}}
\end{align*}
where $q_{ijk} = \E_{\cdot \mid \cdot}(z_{ijk})$ and $\Sigma_{k \, \cdot}\inv$ is
the $k$-th line of $\Sigma\inv$.
The updates for $\bOmega$ are obtained in a similar fashion as before:
\begin{align*}
	\omega_{k,12}^{(l+1)} & = -\left\{(s_{k, 22} + \lambda_k) \left(\Omega_{k, 11}^{(l+1)}\right)\inv + \diag(d_{k, 12})\right\}\inv s_{k, 12}  \\
	\omega_{k, 22}        & = \frac{n_k}{s_{k,22} + \lambda_k} + \left(\omega_{k,12}^{(l+1)}\right)^t \Omega_{k, 11}\inv \omega_{k, 12}^{(l+1)}.
\end{align*}

\section{Adding a prior for Sigma} % TODO: put back \Sigma when you hand it back
Given that $\Sigma$ is meant to represent how similar two graphs are,
we would like such information to be inferred from the data as well, rather than to be imposed by us.
We expand the model in~\eqref{eq:mg-model} by specifying the prior
\[\Sigma \sim \textsc{W}\inv(\Psi, \nu),\]
where $\textsc{W}\inv(\Psi, \nu)$ is the \emph{inverse Wishart} distribution whose density is
\[f(\Sigma; \Psi, \nu) = \frac{\det(\Psi)^\frac{\nu}{2}}{2^\frac{\nu K}{2} \Gamma_p(\frac{\nu}{2})} \det(\Sigma)^{-\frac{\nu + K + 1}{2}} \exp\left\{-\frac{1}{2} \tr(\Psi \Sigma\inv)\right\},\]
with parameters $\Psi$, a positive definite $K \times K$ matrix, and $\nu > K - 1$ a scalar.
The $Q$ function in Equation~\eqref{eq:mg-joint} is now 
\begin{align*}
  Q(\mathbf{\Omega}, \mathbf{\theta}, \Sigma) &=  \E_{\mathbf{z} \mid
	\mathbf{\Omega}\powl, \mathbf{\theta}\powl, \Sigma\powl, \mathbf{Y}}(\log
	p(\mathbf{\Omega},  \mathbf{z}, \mathbf{\theta}, \Sigma \mid \mathbf{Y}) \mid
	\mathbf{\Omega}\powl, \mathbf{\theta}\powl, \Sigma\powl, \mathbf{Y})                    \\
                                              &= Q(\bOmega, \btheta) - \frac{2(\nu + K + 1) + p(p-1)}{4} \log\det(\Sigma) - \frac{1}{2} \tr(\Psi \Sigma\inv) + \text{constant}.
\end{align*}
Adding this new term does not change the computations in the E-step. % TODO: Put the proof of it in the appendix
Let us now compute the posterior distribution of $\Sigma$ in our model
\begin{align*}
  p(\Sigma \mid \bY, \bOmega, \bz, \btheta) &= \frac{p(\bY \mid \bOmega) p(\bOmega \mid \bz) p(\bz \mid \btheta) p(\btheta \mid \Sigma) p(\Sigma)}{p(\bY \mid \bOmega) p(\bOmega \mid \bz) p(\bz \mid \btheta) p(\btheta)} \\
                                            &= \frac{p(\btheta \mid \Sigma) p(\Sigma)}{p(\theta)} \\
                                            &= p(\Sigma \mid \theta).
\end{align*}
Such simplifications are thanks to the fact that $\Sigma$ appears last in the model.
Now, we wish to compute an M-step for $\Sigma$. 
we make use of the fact that the inverse Wishart distribution is conjugate to the multivariate Gaussian.
That is, if we have a matrix $\Theta = [\theta_1, \dots, \theta_{\frac{p(p-1)}{2}}]$ in which each column
is $\Nor_K (0, \Sigma)$ distributed, then the posterior is
\[\Sigma \mid \Theta \sim \textsc{W}\inv\left(\Theta \Theta^t + \Psi, \frac{p(p-1)}{2} + \nu\right).\]
This motivates the update step in which we simply set $\Sigma^{(l+1)}$ to the mode of the posterior distribution
\[\Sigma^{(l+1)} = \frac{\Theta \Theta^t + \Psi}{\frac{p(p-1)}{2} + \nu + K + 1}.\]
In Chapter~\ref{chap:simualtions} we will perform simulations with and without this prior on $\Sigma$.


\chapter{Simulations}\label{chap:simualtions}
In this chapter we show the results of simulations we performed.
\section{Results in the single-graph setting}
We performed a set of simulations with data generated from the R package
\texttt{huge}~\parencite{huge2020}. We compare the results of our method with that of~\cite{mein2006} which for a given node $i$, computes
\[\hat{\theta}^{i, \lambda} = \argmin\displaylimits_{\theta : \theta_i = 0}\frac{1}{n} ||Y_i - Y \theta||_2^2 + \eta ||\theta||_1,\]
where $Y_i$ is the $i$-th column of $Y$, $\theta_j^i = -\omega_{ij}/\omega_{ii}$, and $\eta$ is a constant that controls the $l_1$ penalty term.
We only use one replicate but plan to add more, so we can obtain measures of uncertainty for the performance of our method.
The tests were performed on different graph structures that \texttt{huge} allows us to generate.
In Table~\ref{tab:cheat} and~\ref{tab:honest}, the lines labelled by ``random'' indicate that the
underlying graph was generated such that each edge had an equal probability. For the lines labelled by ``cluster'' instead, vertices were split
into groups and an edge had a higher probability of appearing when the vertices
belonged in the same group.
\begin{table}[ht]
	\centering
	\small
	\begin{tabular}{lcccc}
		\toprule
		graph                    & method & 25   & 50   & 100  \\
		\midrule
		\multirow{2}{*}{random}  & EMGS   & 0.87 & 0.83 & 0.75 \\
		                         & mb     & 0.89 & 0.85 & 0.76 \\
		\midrule
		\multirow{2}{*}{cluster} & EMGS   & 0.73 & 0.70 & 0.67 \\
		                         & mb     & 0.75 & 0.70 & 0.63 \\
		\bottomrule
	\end{tabular}
	\caption{$n=50$, $p \in \{25, 50, 100\}$, F1 scores. For EMGS, hyperparameters were selected by maximising F1 score.}\label{tab:cheat}
\end{table}
\begin{table}
	\centering
	\small
	\begin{tabular}{lcccc}
		\toprule
		graph                    & method & 25   & 50   & 100  \\
		\midrule
		\multirow{2}{*}{cluster} & EMGS   & 0.68 & 0.67 & 0.67 \\
		                         & mb     & 0.89 & 0.87 & 0.89 \\
		\bottomrule
	\end{tabular}
	\caption{$n=200$, $p \in \{25, 35, 50\}$. F1 scores. For EMGS, hyperparameters were selected by maximising the posterior joint distribution.}\label{tab:honest}
\end{table}
Our method, EMGS, estimates $\Omega$ and $\pi$. We use~\eqref{eq:qij} with the
estimates to obtain the posterior probabilities of inclusion of all edges
$(y_i, y_j)$. The method by~\cite{mein2006} estimates $\Omega$ as a function of
$\eta$. Higher $\eta$ values increase the number of zero entries in their
estimate of $\Omega$.
We compare maximal \emph{F1 scores} which is the harmonic mean between \emph{precision}
and \emph{recall}, where the precision is the fraction of true edges
among the edges that the method detects, and recall is the fraction
of true edges which the method detects. These terms are also known as the
positive predictive value and true positive rate.
We fixed $a = b = \lambda = 1, v_1 = 100$ and tried varying
values of $v_0$ between $1e-4$ and $1e-3$.
A better approach will need to be developed in the future.
The choice of $v_0$ and $v_1$ has an impact on the performance of the algorithm.
Indeed, in Table~\ref{tab:cheat} we selected the ``best'' $v_0$ using the ground truth.
Table~\ref{tab:honest} provides a more ``honest'' approach in which we picked $v_0$ by
maximising the posterior distribution.
However, this often produced degenerate selections with the posterior
probabilities of inclusion $p(\delta_{ij} \mid \Omega, \pi, Y)$ collapsing to
either zero or one.

\section{Results in the multi-graph setting}
In this section we show the results of the algorithm derived in Chapter~\ref{chap:multigraph}.

\subsection{Data generation}\label{ssect:data-generation}
The data for the multi-graph setting is obtained by first taking a graph
that was generated with \texttt{huge}.
Then we make a copy of the original graph,
and for each edge we randomly and independently decide if it will be swapped, with some probability.
If yes, we move the edge over to a random pair of unconnected vertices.
We also swap the relevant entries in the precision matrix.
If the resulting precision matrix is not positive definite, we discard it and start over.
This process is repeated $K$ times to obtain the desired number of graphs.

\subsection{Comparison with single-graph methods}
We run the algorithm described in Chapter~\ref{chap:multigraph} with $n=50$ and $p=20$.
In Figure~\ref{fig:omega-mg} we show the entries of the estimated $\hat \Omega_k$ matrix with the multi-graph modlel compared to the true $\Omega_k$ matrix, for $k = 1$.
\begin{figure}[ht]
  \centering
    \includegraphics[width=0.95\textwidth]{pictures/mg1.pdf}
    \includegraphics[width=0.95\textwidth]{pictures/sg1.pdf}
  \caption{A scatterplot comparing the estimated entries of $\Omega$ with the multi-graph model (on top) and the single-graph model (on the bottom). Points in red correspond to true positives, orange are true negatives, blue are false positives, and purple are false negatives. The points have been jittered slightly so that overlapping points are more easily discernible.}
  \label{fig:omega-mg}
\end{figure}

We perform a series of experiments. In the first, we compare the impact
of increasing the number of graphs over the performance of the multi-graph model.
We decide to use $F1$ score as a measure of how good an estimation is.
The graphs are generated in such a way that each edge has a 10\% change of being swapped.
The results are summarized in Table~\ref{tab:mg-more-graphs}. 
Although it's expected, we are pleased to see that the multi-graph model performs better than the other single-graphs method.
Also, the multi-graph model performs better than its single-graph counterpart even when we only have one graph.
Furthermore, the method performs as we expect in the sense that the estimation becomes more accurate as the number of graphs increases.
It is interesting to see that as the number of graphs increases we hit a plateau in terms of the accuracy of the estimation.
This is probably due to the random nature of how the graphs are generated.
It is possible that if there were more interesting links between the graphs then the estimations could improve.

\begin{table}[ht]
  \caption{F1 scores as the number of graphs increases, for $n=50$ and $p=20$.
  The leftmost two columns (M+B, and SG) are single-graph methods,
they were only estimating the first graph out of $K$, and are acting as a sort of control.
The column ``M+B'' denotes~\cite{mein2006} method.
The column ``SG'' denotes the single-graph method from~\cite{limcco-2017}.
The columns ``MG'' denote the multigraph method from Chapter~\ref{chap:multigraph} in which we use an increasing number of graphs, $K$.
The rows correspond to how the graph was generated. 
Random means that each edge indipendently had a given probability of existing, 
and scale-free means that the created graph has the scale-free property.
Each entry averaged over five runs, in which the same five arbitrarily chosen seeds were used.}
  \label{tab:mg-more-graphs}
  \begin{center}
    \begin{tabular}[c]{l|cccccc}
      \toprule
      Graph & M+B & SG & MG ($K = 1$) & MG ($K = 2$) & MG ($K = 5$) & MG ($K = 10$) \\
      \midrule
      Scale-free & 0.47 & 0.37 & 0.52 & 0.59 & 0.63 & 0.63 \\
      \midrule
      Random & 0.57 & 0.44 & 0.51 & 0.62 & 0.65 & 0.65 \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

In the second experiment we fix $K=4$ and instead we increase the dissimilarity between graphs.
The results are summarized in Table~\ref{tab:mg-more-random}. 
Here we only look at the multi-graph model.
We see that in general the model performs similarly,
regardless of how different the graphs are from eachother.
This is once again possibly due to the random nature of how the graphs are generated.
% We find it quite odd that there is a difference in the model's performance
% between when the edge-swapping probability is zero and when we only have a single graph.
% Indeed there should not be any difference, because in the former case
% we only have the same graph $K$ times, which should not give any new information,
% and therefore the performance should be similar as when we apply the model to a single graph.

\begin{table}[ht]
  \caption{F1 scores as the dissimilarity between graphs increases, for $n=50$, $p=20$, and $K=4$.
    The factor ``$prob$'' gives how likely an edge will be swapped when we generate the graphs
    (refer to Subsection~\ref{ssect:data-generation}). 
    In particular, if $prob = 0.0$, we obtain the same graph several times.
    The rows correspond to how the graph was generated.
    Random means that each edge indipendently had a given probability of existing, 
    Each entry averaged over five runs, in which the same five arbitrarily chosen seeds were used.
  }
  \label{tab:mg-more-random}
  \begin{center}
    \begin{tabular}[c]{l|cccccc}
      \toprule
      Graph  & $prob = 0.05$ & $prob = 0.1$ & $prob = 0.2$ & $prob = 0.5$ & $prob = 1.0$ \\
      \midrule
      Random  & 0.634 & 0.646 & 0.638 & 0.656 & 0.642 \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

\subsection{Oddities regarding the likelihood}
When doing our experiments we noticed that the $Q(\bOmega, \btheta, \Sigma)$ (which we call likelihood here) function from Chapter~\ref{chap:multigraph}
is not getting optimized. This is strange given that the algorithm does produce satisfactory results.
We have found that the likelihood will increase very quickly in the beginning iterations,
but then it will decrease and settle at a lower value. This can be seen in Figure~\ref{fig:loglik-mg-1}.
In particular the decrease is sharper and reaches a lower point the larger the number of graphs we are using.
This happens even when we do not use a prior on $\Sigma$.
In the best case scenario it is simply a miscalculation in our code.
Otherwise, this could possibly be due once again to how we generate our data, or worse,
it could mean that the model is misspecified and some other approach would be more beneficial.
Unfortunately, we do not have a particularly convincing explanation of why this phenomenon happens.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=.45\textwidth]{pictures/loglik-mg-1.pdf}
    \includegraphics[width=.45\textwidth]{pictures/loglik-mg-2.pdf}
    \hfill
  \end{center}
  \caption{Evolution of log likelihood over iterations of fitting the multi-graph model, $p=20$, and $n=50$. On top we have $K=4$ graphs, on the bottom $K=15$.}
  \label{fig:loglik-mg-1}
\end{figure}
The code for the simulations can be found on
\href{https://github.com/jkasalt/pdm_summary}{this github repository}.

\chapter{Choosing hyperparameters}
This chapter mainly concerns the choice of $v_0$ in the single-graph model.
The performance of the model is heavily influenced by the values of the
hyperparameters.
For the multiple graphs setting, we run one one the methods described below for each graph individually
to obtain multiple values of $v_0$ with which we fit the mutli-graph model.
 On Figure~\ref{fig:mean_auc} we have plotted the mean AUC for
changing values of $v0$ in the single graph setting.
\begin{figure}[ht]
	\centering
	\includegraphics[width=.7\textwidth]{pictures/Rplot01.png}
	\caption{Mean AUC over ten replicates for different values of $v_0$, with a
		graph with 25 nodes and 100 samples.}\label{fig:mean_auc}
\end{figure}
To solve this problem there are two main methods that we have found.

\section{Imposing sparsity}\label{sect:imposing-sparsity}
Suppose it is known that the proportion of edges has to be some value $s \in [0,1]$.
Then, since the parameter $\pi$ controls the prior sparsity of the graph, we 
fix $a = 1$ and $b$ so that the mean of $\pi$, $a / (a+b) = s$. 
We then run the single-graph model over a grid of $v_0$ values and pick the one 
for which the estimated graph has edge density closest to the $s$ we imposed.
On Figure~\ref{fig:auc} we show the result of choosing $v_0$ with this method.
Note that we used the true sparsity for that graph, which was approximately 0.15.
It is interesting to see that despite using the true sparsity, we overestimate
$v_0$.
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.95\textwidth]{pictures/auc1.pdf}
  \end{center}
  \caption{Area under curve (``AUC'') values for single-graph estimation as a function of $v_0$, with $n=50$, $p=20$.
    The red line represents the value for $v_0$ we would choose if we followed the method outlined in Section~\ref{sect:imposing-sparsity},
    and the orange line the one we would choose with the method in Section~\ref{sect:graphical-aic}.
}
  \label{fig:auc}
\end{figure}

We see that the choice of sparsity $s$ is critical for this method, but often it is 
not obvious what a good choice may be. Here we display an unsuccessful attempt at
finding a good value for $s$, in the hopes that a follow-up discussion could help
move this issue forward.
It is only possible to do this if we have a sample size that is larger than the number of parameters,
but we can investigate the entries of the empirical precision matrix and make a histogram out of them.
We obtain it by computing the empirical covariance matrix, $\bar \Omega$ and inverting it.
Given that, we would like to find a distribution that looks like a spike-and-slab prior. 
At that point we can look for some threshold $t \in \mathbb{R}$, so that inside the interval
$[-t, t]$ the spike is larger than the slab, and outside the slab is larger.
Then we would set the sparsity $s$ to the proportion of entries that are inside $[-t, t]$.
In practice this method was unsuccessful because it is too reliant on having the data generated exactly in the way we predict.
Therefore it was hard to find $t$ given that often the histogram of the upper triangle of $\bar \Omega$ 
simply did not look like a spike and slab. Attempting to fit a Gaussian mixture model did not work either.

\section{Graphical AIC}\label{sect:graphical-aic}
A method that worked well instead was to use the Akaike information criterion (AIC). 
The AIC in the graphical setting is given by the following formula
\[2|E| - \log\det(\Omega) + \tr(S \Omega),\]
where $S = YY^t$ and $Y = [y_1, \dots, y_n]$.
As we can see in Figure~\ref{fig:aic} the $v_0$ value that minimizes AIC is the one which maximizes
the AUC.
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.95\textwidth]{pictures/auc2.pdf}
  \end{center}
  \caption{AIC values for single-graph estimation as a function of $v_0$, with $n=50$, $p=20$.
  The green line represents the value of $v_0$ for which the area under curve (``AUC'') is maximized.
}
  \label{fig:aic}
\end{figure}
Though here we only show the result for one graph, using the AIC in this way generally gives
$v_0$ values that are very close to the best $v_0$.
There are other formulas we could have used such as the Bayes information criterion (BIC), or the
extended BIC, but we were generally satisfied enough with the results of the AIC that we did not
feel the need to use something else.

\chapter{Discussion and further work}
We have derived a conditional expectation maximization algorithm that performs inference on multiple graphs
which outperforms current implementations. Using Bayesian priors, we were able to pool information across
graphs to improve our estimates. 
For future work, we would like to solve the strange inconsistencies found during the simulations, or at 
least to have an explanation of why they happen.
Also we would like to apply our estimation procedure to a real dataset.

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography
\end{document}
