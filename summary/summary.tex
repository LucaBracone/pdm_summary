\documentclass[a4paper, 11pt, oneside]{report}
\usepackage[MScThesis]{EPFLreport}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{booktabs}

\usepackage{tikz}

\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\cof}{cof}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\argmin}{argmin}
\newcommand{\iid}{\stackrel{\textmd{i.i.d.}}{\sim}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\1}{\mathds{1}}
\newcommand{\powl}{^{(l)}}
\newcommand{\inv}{^{-1}}
\newcommand{\Np}{\textsc{N}_p}
\newcommand{\Nor}{\textsc{N}}
\newcommand{\sns}{\textsc{sns}}
\newcommand{\Bern}{\textsc{Bern}}
\newcommand{\Beeta}{\textsc{Beta}}
\newcommand{\Exp}{\textsc{Exp}}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\nindep}{{\rotatebox[origin=c]{90}{$\nvDash$}}}
\newcommand{\bOmega}{\mathbf{\Omega}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\btheta}{\mathbf{\theta}}
\newcommand{\tijkl}{\theta_{ijk}\powl}
\newcommand{\bY}{\mathbf{Y}}

\author{Luca Bracone}
\title{Bayesian joint inference for multiple graphical models using
	spike-and-slab priors}

\begin{document}
\maketitle
% \makededication
% \makeacks

% TODO: Augment abstract with "why scaleable?", "why multigraph?", talk about our context
\begin{abstract}
	Graphical models are models in which the conditional independence of
	variables is represented as a graph. If the variables are assumed to  follow
	a multivariate Gaussian distribution, conditional
	independence between pairs of variables is equivalent to the corresponding entries of the precision matrix being zero. Using a
	spike-and-slab prior we aim to model the zero and
	non-zero entries as a mixture. Rather than using Gibbs-sampling methods, our work makes
	use of an expectation conditional maximisation algorithm (ECM) in order to
	obtain fast pointwise estimates. We extend previously done work by focusing
	on the analysis of multiple graphs. In doing so we leverage shared
	information across graphs to obtain better estimates. We show on simulated
	data that our method produces better estimates than other single-graph
	methods. Our method is designed to scale well in highly dimensional data.

\end{abstract}

\chapter{Introduction}
In recent times, there has been an increased interest in finding complex
relationships underlying biological processes, such as gene expression pathways
or connections between neurons in the brain. In the past, many approaches have
focused on \emph{directed graphical models}, in which nodes represent random
variables and the structure of edges forces the joint distribution to factor in
a certain way. Some approaches have instead focused on \emph{undirected
	graphical models}, in which the nodes represent also some variables of interest, but
in which the existence of edges imposes a certain conditional independence
structure on the variables. This report develops methods to infer edges in an
undirected graph.

We use a Bayesian framework. This allows us to specify a prior distribution over the
graphs, which can encode specific domain knowledge. For instance, the estimated
graph is often chosen to be sparse, i.e.,\ to have few edges.
There are many possible priors one can choose from.
The conjugate $G$-Wishart prior derived by~\citet{HyperInverseWRovera2002}
has been a common choice.
Recently, other priors have been used because they have
better computational scalability as the number of parameters increases.
Those include the graphical horseshoe~\citep{yunf2019}, the spike-and-slab
graphical lasso~\citep{limm2018}, and the
spike-and-slab prior~\citep{wang-2015} which is the one we will use.

Most inference approaches for undirected Bayesian graphs have focused on
stochastic methods which obtain an estimate of the full posterior distribution
using numerical sampling methods such as Markov chain Monte Carlo (MCMC) with
Gibbs sampling. However, for many practical applications
point estimates are sufficient, so we will follow~\citet{limcco-2017}, who
derive an expectation conditional maximisation (ECM) approach to inference. We
will then extend their method to multiple graphs.~\citet{luke2017} have a method that is
fairly similar to ours, but we will use a probit link to pool
information across the graphs (they use a logistic link), and we will also infer
pairwise similarities between the graphs.
In Chapter~\ref{chap:graphs} we will cover the basics of Bayesian hierarchical and graphical models. In Chapter~\ref{chap:multigraph} we will derive the multi-graph model, and describe an ECM algorithm to fit it. Finally, in Chapter~\ref{chap:simualtions} we will highlight methods to choose hyperparameters, perform simulations, and assess the performance of the multi-graph model.

\chapter{Undirected graphical models for multivariate Gaussian
  variables}\label{chap:graphs}

\section{Bayesian hierarchical models}
A hierarchical model involves conditional prior distributions over all model parameters
$\theta_1, \dots, \theta_p$. This results in a hierarchical structure, and the
higher a parameter is placed
up the hierarchy, the higher the number of samples we need to have to produce
confident estimates of it.
For example, suppose that we observe values $y_{ij} \iid P(\theta_j)$ where $j=1,\dots,p$ and
$i=1,\dots,n_j$. That is, we have a certain number of observations,
each belonging to some group, and the values within each group have
parameter $\theta_j$. We will now discuss two methods to analyze such data
that will motivate the use of hierarchical models.

It might at first seem appealing to ignore the differences between groups. This
means setting all the $\theta_j$ to be equal to each other, so we can perform
usual maximum likelihood estimation. If there is a group with only a few
outlying observations, the maximum likelihood estimator will have high bias, and it will have an estimate that is far from its observations.
Then another idea might be to treat each group in a separate estimation procedure, assuming
that they are unrelated. In some cases, this is
justified, but the group with only a few
observations will have an estimate with a large variance.

A hierarchical model sees the first two methods as two
extremes: ``the $\theta_j$ are the same'' and ``the $\theta_j$ are unrelated''.
Is there a way to automatically decide how similar or how different the $\theta_j$ should be from each other?
Yes, we will imagine that the $\theta_j$ are themselves independent and identically distributed
$\theta_j \mid \phi \iid Q(\phi)$, for some parameter $\phi$. Then the posterior
joint distribution can be expressed as
\[p(\phi, \theta_1, \dots, \theta_p \mid y) \propto p(y \mid \theta) p(\theta_1, \dots, \theta_p \mid \phi) p(\phi).\]
The distribution $p(\phi)$ chosen for $\phi$ is often
referred to as a \emph{hyperprior}.

\section{Undirected graphical models}
An undirected graphical model is a model in which the conditional
structure of some variables of interest $y_1, \dots, y_p$ are represented using a
graph $G=(V, E)$, with $V = \{y_1, \dots, y_p\}$ such that an edge $(y_i, y_j)$
exists in $E$ if and only if $y_i$ and $y_j$ are dependent given all the other
variables (which we denote as $y_{-(ij)}$, for $i,j \in \{1,\dots,p\}$). So in summary
\[y_i \nindep y_j \mid y_{-(ij)} \iff (y_i, y_j) \in E.\]
For a sample
of $n$ $p$-dimensional multivariate Gaussian variables $y^1, \dots, y^n \sim
	\Np(0, \Sigma)$ we would like to deduce the structure of the graph $G$ by
estimating the \emph{precision matrix} $\Omega =
	\Sigma\inv$ and by observing that a given entry $\omega_{ij}$ is zero if and only if $y_i$ and $y_j$ are
independent given $y_{-(ij)}$, i.e.,
\[(y_i, y_j) \notin E \iff \omega_{ij} = 0.\]

\section{Spike and slab prior for graphical models}
Let $y \in \R^p$ be a random vector distributed under the hierarchical model
\begin{align*}
	y \mid \Omega                & \sim \Np(0, \Omega\inv),\quad \Omega \in M^+,                                                                                   \\
	\omega_{ij} \mid \delta_{ij} & \sim \delta_{ij} \Nor(0, v_1^2) + (1 - \delta_{ij}) \Nor(0, v_0^2), \quad v_0^2 \ll v_1^2,\quad i,j = 1,\dots,p,\quad i \neq j, \\
	\omega_{ii}                  & \sim \textsc{Exp}(\lambda / 2),                                                                                                 \\
	\delta_{ij} \mid \pi         & \sim \Bern(\pi),                                                                                                                \\
	\pi                          & \sim \Beeta(a,b),
\end{align*}
where $M^+$ is the set of symmetric positive definite matrices, $\Np(0, \Omega\inv)$ is the multivariate normal distribution with mean $0$ and
covariance matrix $\Omega\inv$, and $a, b, \lambda, v_0^2, v_1^2 \in \R^+$ are hyperparameters.
The entries $\omega_{ij}$ are such that the conditional distribution of $\Omega$ as a whole can be written as
\[p(\Omega \mid \delta) = C\inv \prod_{i < j}  \Nor(\omega_{ij} \mid 0,
	v_{\delta_{ij}}^2) \prod_i \Exp\left(\omega_{ii} \mid \frac{\lambda}{2}\right) \1\{\Omega \in M^+ \},\]
with $C$ a constant that depends on $\delta, v_0^2, v_1^2$, and $\lambda$.
This is known as the continuous \emph{spike-and-slab} prior because it
corresponds to a mixture of two Gaussian distributions, one with a small variance
$v_0^2$ (the spike) and one with a large variance $v_1^2$ (the slab).
Under this model, if an entry $\omega_{ij}$ is truly zero,
it is absorbed in the spike and estimated as close to zero. The
discrete spike-and-slab instead uses a point mass at zero, $\1\{\omega_{ij} =
	0\}$. We use the former because it makes the computation of the posterior
distribution simpler. % TODO: elaborate
Finally, let $Y \in \R^{n \times p}$ be the matrix whose rows are identically and independently distributed
observations of $y$. We seek values of $\Omega, \delta, \pi$ that
maximise the log posterior joint distribution $\log p(\Omega, \delta, \pi \mid
	Y)$. The posterior joint distribution can be
decomposed as
\begin{equation}\label{eq:decomp}
	p(\Omega, \delta, \pi \mid Y) = p(\Omega \mid \delta) p(\delta \mid \pi) p(Y \mid
	\Omega) p(\pi) p(Y)\inv.
\end{equation}
The factor $p(Y)\inv$ is constant, and hence has no influence on the
maximisation. Using the definitions and the decomposition of~\eqref{eq:decomp}, we find that
$\log p(\Omega, \delta, \pi \mid Y)$ equals
\begin{multline}\label{eq:joint}
	\text{constant} + \sum_{i<j} \left[-\frac{1}{2}\log\left\{v_0^2 (1 - \delta_{ij}) +
	v_1^2 \delta_{ij}\right\} - \frac{\omega^2_{ij}}{2} \frac{1}{v_0^2 (1 -
		\delta_{ij}) + v_1^2 \delta_{ij}} \right] - \sum_i \frac{\lambda}{2}
	\omega_{ii}\\
	+ \sum_{i < j} \left\{\delta_{ij} \log(\pi) + (1 - \delta_{ij})\log(1 - \pi)\right\}                                                                                                                \\
	+ (a-1) \log(\pi) + (b-1)\log(1 - \pi) + \frac{n}{2} \log\det(\Omega) -\frac{1}{2}\tr (Y^t Y \Omega).
\end{multline}

\section{Expectation-Maximisation for Bayesian graphical models}
Following~\citet{limcco-2017} instead of maximising~\eqref{eq:joint}, we
iteratively maximise its expectation over $\delta$, which we implement using an iterative ``expectation conditional maximization'' (ECM) approach. Taking the expectation
of~\eqref{eq:joint} we obtain
\begin{equation}\label{eq:cond}
	Q(\Omega, \pi \mid \Omega\powl, \pi\powl, Y) = \mathbb{E}_{\delta \mid
		\Omega\powl, \pi\powl, Y}\left\{\log p(\Omega, \delta, \pi \mid X) \Bigm\vert
	\Omega\powl, \pi\powl, Y\right\},
\end{equation}
where $\Omega\powl$ and $\pi\powl$ denote the values obtained for $\Omega$ and
$\pi$ at the $l$-th iteration of the algorithm, respectively.
Equation~\eqref{eq:cond} is equal to
\begin{multline}
	\text{constant} - \sum_{i<j} \frac{\omega_{ij}^2}{2} \E_{\delta_{ij} \mid
		\cdot}\left(\frac{1}{v_0^2 (1 - \delta_{ij}) + v_1^2 \delta_{ij}}\right) -
	\sum_i \frac{\lambda}{2} \omega_{ii}
	\\
	+ \frac{p(p-1)}{2}  \log(1 - \pi) + \sum_{i<j} \E_{\delta_{ij} \mid
		\cdot}(\delta_{ij}) \log\left(\frac{\pi}{1-\pi}\right)
	\\
	+ (a - 1) \log(\pi) + (b - 1) \log(1 - \pi)
	+ \frac{n}{2} \log\det(\Omega) - \frac{1}{2} \tr(Y^t Y \Omega),
\end{multline}
where $\mathbb{E}_{\delta_{ij} \mid \cdot}$ denotes the conditional expectation with respect
to $\delta_{ij} \mid \Omega\powl, \pi\powl, Y$.
The expectation terms are
\begin{equation}\label{eq:qij}
	\E_{\delta_{ij} \mid \cdot}(\delta_{ij}) = p\left(\delta_{ij} = 1 \mid \omega_{ij}\powl, \pi\powl\right)
	= \frac{ \pi\powl p\left(\omega_{ij}\powl \mid \delta_{ij} = 1\right)}{\pi\powl p\left(\omega_{ij}\powl \mid \delta_{ij} = 1\right) +
		\left(1 - \pi\powl\right) p\left(\omega_{ij}\powl \mid \delta_{ij} = 0\right)},
\end{equation}
which we denote by $q_{ij}$, and
\begin{equation}\label{eq:dij}
	d_{ij} := \E_{\delta_{ij} \mid \cdot}\left(\frac{1}{v_0^2 (1 -
			\delta_{ij}) + v_1^2 \delta_{ij}}\right) = \sum_{\delta = 0}^1
	\frac{p\left(\delta \mid \omega_{ij}\powl, \pi\powl\right)}{v_0^2 (1 - \delta) +
		v_1^2 \delta} = \frac{q_{ij}}{v_1^2} + \frac{1 - q_{ij}}{v_0^2}.
\end{equation}
This is the \emph{expectation step} (E step).
Now we use~\eqref{eq:qij} and~\eqref{eq:dij} to compute the next iterates $\pi^{(l+1)}$ and $\Omega^{(l+1)}$.
The derivative of~\eqref{eq:cond} with respect to $\pi$ is
\[\pi\left\{\frac{p(p-1)}{2} - a - b + 2\right\} + \sum_{i<j} q_{ij} + a - 1,\]
and it is equal to zero when \[\pi = \frac{a-1 + \sum_{i<j}q_{ij}}{a + b - 2 +
	\frac{p(p-1)}{2}}.\]
The maximisation with respect to $\Omega$ requires that $\Omega$ remains positive definite
after each iteration. In the context of Gibbs sampling,~\citet{wang-2015} has shown that if we
slice the matrices $\Omega$, $Y^t Y$ and $V = {(v_{\delta_{ij}})}_{ij}$ in the following way
\[\Omega = \begin{pmatrix}
		\Omega_{11}   & \omega_{12} \\
		\omega_{12}^t & \omega_{22}
	\end{pmatrix},
	\quad
	Y^t Y = \begin{pmatrix}
		S_{11}   & s_{12} \\
		s_{12}^t & s_{22}
	\end{pmatrix},
	\quad
	V = \begin{pmatrix}
		V_{11}   & v_{12} \\
		v_{12}^t & v_{22}
	\end{pmatrix},
\]
where $\omega_{22}$ is a scalar and $\omega_{12}$ is a $(p-1)$-dimensional
vector (likewise for $s_{22}$, $s_{12}$, and $v_{12}, v_{22}$), we find the
conditional distributions
\[
	\omega_{12} \mid \delta, Y \sim \Nor(-C s_{12}, C) \quad C=\left\{(s_{22} +
	\lambda) \Omega_{11}^{-1} + \diag\left(v_{12}^{-1}\right)\right\}\inv,
\]
and
\[\omega_{22} \mid \delta , Y \sim \textsc{Gamma}\left(\frac{n}{2} + 1, \frac{s_{22} +
		\lambda}{2}\right) + \omega_{12}^t\Omega_{11}^{-1}\omega_{12}.\]
The term $v_{12}^{-1}$ refers to the vector $v_{12}$ after we inverted each
component, so $\E\left(v_{12}^{-1}\right) = d_{12}$, where $d_{12}$ is the
vector of $d_{ij}$ values defined similarly to $\omega_{12}$.
Taking the mode of these distributions gives
\begin{align*}
	\omega_{12}^{(l+1)} & = -{\left\{(s_{22} + \lambda) \Omega_{11}^{-1} + \diag(d_{12})\right\}^{-1}} s_{12},                   \\
	\omega_{22}^{(l+1)} & = \frac{n}{s_{22} + \lambda} + \left(\omega_{12}^{(l+1)}\right)^t \Omega_{11}^{-1}\omega_{12}^{(l+1)},
\end{align*}
which results in a \emph{conditional maximisation step} (CM-step).
Inference is performed by alternating between the E and CM steps, until convergence of $Q$, for some prespecified tolerance.

\chapter{Extension to multiple graphs}\label{chap:multigraph}
\section{The multi-graph model}
In some applications, it is reasonable to assume that we have two or more different graphs (for instance, diseased and healthy patients).
So now, we suppose that each sample belongs to one of $K$ groups, where $K$ is the number of graphs.
This means that we have $K$ sets $\left\{y_k^1, \dots, y_k^{n_k}\right\}$, each of which we assume to be realizations of a multivariate Gaussian distribution \[y_k \mid \Omega_k \sim \Np(0, \Omega_k\inv), \quad \Omega_k \in M^+,\] where $M^+$ is the set of positive-definite matrices.
We would like to make use of a hierarchical model to pool information across $\Omega_1,\dots,\Omega_K$.
We will use the model
\begin{align*}
	y_k \mid \Omega_k              & \sim \Np(0, \Omega_k^{-1}),\quad \Omega_k \in M^+, \quad k=1,\dots,K,                                                                               \\
	\omega_{ijk} \mid \delta_{ijk} & \sim \delta_{ijk} \Nor(0, v_{1,k}^2) + (1 - \delta_{ijk}) \Nor(0, v_{0,k}^2),  \quad v_{0,k}^2 \ll v_{1,k}^2, \quad i,j = 1,\dots,p,\quad i \neq j, \\
	\omega_{iik}                   & \sim \Exp(\lambda_k/2),                                                                                                                             \\
	\delta_{ijk} \mid \theta_{ijk} & \sim \Bern(\Phi(\theta_{ijk})),                                                                                                                     \\
	\theta_{ij}                    & \sim \textsc{N}_K(\theta_0, \Sigma_0),\stepcounter{equation}\tag{\theequation}\label{eq:mg-model}
\end{align*}
where $\Phi$ is the standard normal cumulative distribution function, $v_{0,k}^2,
	v_{1,k}^2, \lambda_k \in \mathbb{R}^+$, $\Sigma_0 \in M^+$ are hyperparameters, and $\theta_0 < 0$ is a hyperparameter to induce sparsity.
In Section~\ref{sec:hyperparameters} we discuss how these parameters are chosen.
We use the following data augmentation for inference,
\[\delta_{ijk} = \1\left\{z_{ijk} > 0\right\}, \quad \text{where } z_{ijk} \mid \theta_{ijk} \sim N\left(\theta_{ijk}, 1\right).\]
Let $Y_k$ denote the matrix whose rows are observations of $y_k$, and let $Y$
denote $\{Y_1,\dots,Y_K\}$.
The posterior joint distribution $p(\Omega,  z, \theta \mid Y)$ decomposes as
\begin{align}\label{eq:decomp-join}
	p(\Omega,  z, \theta \mid Y) & = p(Y \mid \Omega) p(\Omega \mid z)
	p(z \mid \theta) p(\theta) p(Y)\inv \nonumber                       \\
	                             & = \prod_{k=1}^K p(Y_k \mid \Omega_k)
	\prod_{i < j}\prod_{k=1}^K p(\omega_{ijk}
	\mid z_{ijk}) \prod_{i < j}\prod_{k=1}^K
	p(z_{ijk} \mid \theta_{ijk})\prod_{i <
		j} p(\theta_{ij}) p(Y)\inv.
\end{align}
When we take the log of~\eqref{eq:decomp-join} and unravel the formula we obtain
\begin{multline}\label{eq:mg-joint}
	- \frac{Kp(p-1)}{2} \log(2\pi) - \frac{p(p-1)}{4} \log\det(\Sigma_0) + \sum_{k=1}^K \frac{n_k}{2} \log\det(\Omega_k) - \sum_{k=1}^K \frac{p n_k}{2} \log(2 \pi)  \\
	-\sum_{k=1}^K\frac{1}{2} \tr(S_k \Omega_k) + \sum_{i < j}\sum_{k=1}^K -\frac{1}{2} \log(2 \pi v_{\delta_{ijk}, k}) - \sum_{i < j} \sum_{k=1}^K\frac{\omega_{ijk}^2}{2 v_{\delta_{ijk}, k}^2} + \sum_{k=1}^K p \log\left(\frac{\lambda_k}{2}\right) - \sum_{k=1}^K \frac{\lambda_k}{2} \tr(\Omega_k)  \\
	-\frac{1}{2} \sum_{i < j}\sum_{k=1}^K (z_{ijk} - \theta_{ijk})^2  -\frac{1}{2} \sum_{i < j} (\theta_{ij} - \theta_0)^t \Sigma_0\inv (\theta_{ij} - \theta_0) - \log p(Y),
	% \stepcounter{equation}\tag{\theequation}\label{eq:mg-joint}
\end{multline}
where $S_k = Y_k^t Y_k$.
When we take the conditional expectation of~\eqref{eq:mg-joint} over the latent
variable $z_k$, we obtain the objective function we will maximize with the ECM algorithm
\begin{align*}
	Q(\Omega, \theta \mid \Omega\powl, \theta\powl, Y) & = \E_{z \mid \Omega\powl, \theta\powl, Y} \left\{\log p(\Omega,  z, \theta \mid Y) \mid \Omega\powl, \theta\powl, Y\right\}                                                                                                                      \\
	                                                   & = \text{constant} + \sum_{k=1}^K \frac{n_k}{2} \log \det(\Omega_k) - \frac{1}{2} \sum_{k=1}^K \tr(S_k \Omega_k) - \frac{1}{2} \sum_{i < j} \theta_{ij}^t \Sigma_0\inv \theta_{ij}                                                                \\
	                                                   & - \frac{1}{2} \sum_{i < j} \sum_{k=1}^K \omega_{ijk}^2 \E_{z_{ijk} \mid \cdot} \left\{\frac{1}{\delta_{ijk} v_{1,k}^2 + (1 - \delta_{ijk}) v_{0,k}^2}\right\}                                                                                    \\
	                                                   & - \frac{1}{2} \sum_{k=1}^K \lambda_k \tr(\Omega_k)  -\frac{1}{2} \sum_{i < j} \sum_{k = 1}^K \theta_{ijk}^2 + \sum_{i < j} \sum_{k=1}^K \theta_{ijk} \E_{z_{ijk} \mid \cdot}(z_{ijk}) ,\stepcounter{equation}\tag{\theequation}\label{eq:mg-exp}
\end{align*}
where $\E_{z_{ijk} \mid \cdot}$ refers to the expectation of $z_{ijk}$ conditioned on
$\Omega\powl, \theta\powl$, and $Y$.
We now proceed to the computation of the expectation terms.
First we note that
\begin{align}
	p(\delta_{ijk} = 1 \mid Y, \Omega\powl, \theta\powl) & = \frac{\Nor\left(\omega_{ijk}\powl \mid 0,
		v_{1,k}^2\right)\Phi\left(\theta_{ijk}\right)}{\Nor\left(\omega_{ijk}\powl \mid 0,
		v_{0,k}^2\right)\{1 -
		\Phi\left(\theta_{ijk}\right)\} +
		\Nor\left(\omega_{ijk}\powl \mid 0, v_{1,k}^2\right)
		\Phi\left(\theta_{ijk}\right)}\label{eq:pdelta1}
\end{align}
Then the first expectation term is given by
\begin{align*}
	\E_{\cdot \mid \cdot} \left\{ \frac{1}{v_{0,k}^2 (1 - \delta_{ijk}) + v_{1,k}^2 \delta_{ijk}} \right\}
	% & = \E_{\cdot \mid \cdot} \left\{ \frac{1}{v_{0,k}^2 ( 1 - \delta_{ijk}) + v_{1,k}^2 \delta_{ijk}} \right\} \\
	= \frac{p\left(\delta_{ijk} = 1 \mid Y, \Omega\powl, \theta\powl\right)}{v_{1,k}^2}
	+ \frac{1 - p\left(\delta_{ijk} = 1 \mid Y, \Omega\powl,
		\theta\powl\right)}{v_{0,k}^2}.
\end{align*}
To calculate the other expectation term, $\E_{z_{ijk} \mid \cdot}(z_{ijk})$, we
first see that
\begin{align*}
	p \left(z_{ijk} \mid Y, \Omega\powl, \delta_{ijk}, \theta\powl\right) & = \frac{p\left(z_{ijk}, Y, \Omega\powl, \delta_{ijk}, \theta\powl\right)}{p\left(Y, \Omega\powl, \delta_{ijk}, \theta\powl\right)}                                                                                                                                                                                                                 \\
	                                                                      & = \frac{p\left(Y \mid \Omega\powl\right) p\left(\Omega\powl \mid z_{ijk}\right) p\left(z_{ijk} \mid \delta_{ijk}\right) p\left(\delta_{ijk} \mid \theta\powl\right) p\left(\theta\powl\right)}{p\left(Y \mid \Omega\powl\right) p\left(\Omega\powl \mid \delta_{ijk}\right) p\left(\delta_{ijk} \mid \theta\powl\right) p\left(\theta\powl\right)} \\
	                                                                      & = p(z_{ijk} \mid \delta_{ijk}).
	% \stepcounter{equation}\tag{\theequation}\label{eq:cond-z}
\end{align*}
If $\delta_{ijk} = 1$ then $z_{ijk} \mid \delta_{ijk}$ is the same as $z_{ijk}
	\mid z_{ijk} > 0$ which is a truncated normal random variable with mean
\[\theta_{ijk}\powl + \frac{\phi(\theta_{ijk}\powl)}{\Phi(\theta_{ijk}\powl)},\]
where $\phi$ denotes the PDF of a standard normal random variable.
On the other hand, if $\delta_{ijk} = 0$ then $z_{ijk} \mid \delta_{ijk}$ is the
same as $z_{ijk} \mid z_{ijk} \leq 0$ which is also a truncated normal random
variable with mean
\[
	\theta_{ijk}\powl - \frac{\phi(\theta_{ijk}\powl)}{1 - \Phi(\theta_{ijk}\powl)}.
\]
Therefore
\begin{align*}
	\E_{z_{ijk} \mid \cdot}(z_{ijk}) & = \sum_{\delta_0 = 0}^1 \E_{\cdot \mid
		\cdot}\left(z_{ijk} \mid \delta_{ijk} = \delta_0, y_k, \Omega_k, \theta_{ijk}\right)
	p(\delta_{ijk} = \delta_0 \mid y_k, \Omega_k, \theta_{ijk})                                              \\
	                                 & = \left\{\tijkl +
	\frac{\phi(\tijkl)}{\Phi(\tijkl)}\right\}
	p(\delta_{ijk} = 1 \mid \cdot) + \left\{\theta_{ijk}\powl -
	\frac{\phi(\theta_{ijk}\powl)}{1 -
	\Phi(\theta_{ijk}\powl)}\right\}p(\delta_{ijk} = 0 \mid \cdot) \label{eq:expect-z}                       \\
	                                 & = \tijkl - \frac{\phi(\tijkl)}{1 - \Phi(\tijkl)} + p(\delta_{ijk} = 1
	\mid \cdot) \left\{\frac{\phi(\tijkl)}{\Phi(\tijkl)} + \frac{\phi(\tijkl)}{1
	- \Phi(\tijkl)}\right\}                                                                                  \\
	                                 & = \tijkl + M\left(\tijkl, 0\right) + p(\delta_{ijk} = 1 \mid
	\cdot)\left\{M\left(\tijkl, 1\right) -
	M\left(\tijkl, 0\right)\right\},
\end{align*}
where $M(\alpha, c)$ denotes Mill's ratio
\[
	M(\alpha, c) = (-1)^{1-c}\frac{\phi(\alpha)}{\Phi(\alpha)^c \{1 - \Phi(\alpha)\}^{1-c}}.
\]
This is the E-step for the multi-graph setting.
Now for the M-step, we write $\Xi = \Sigma_0\inv$ and let $\xi_{kk'}$ be the $(k, k')$-th entry of $\Xi$.
We differentiate $Q(\Omega, \theta \mid \Omega\powl, \theta\powl, Y)$ with respect to
$\theta_{ijk}$ to obtain
\begin{equation}\label{eq:dQdtheta}
	q_{ijk} - \theta_{ijk}  - \xi_{kk} \theta_{ijk} - \sum_{\substack
		{k' = 1 \\ k' \neq k}}^K \xi_{k k'} \theta_{ijk'} + \sum_{k'=1}^K \theta_{0,k'} \xi_{k'k}.
\end{equation}
Equation~\eqref{eq:dQdtheta} is equal to
zero when
\begin{align*}
	\theta_{ijk} & = \frac{q_{ijk} + \sum_{k'=1}^K \theta_{0,k'} \xi_{k'k} - \sum_{\substack{k'=1 \\ k' \neq k}}^K \xi_{k k'}\theta_{ijk'}}{ 1 + \xi_{kk}},
\end{align*}
where $q_{ijk} = \E_{z_{ijk} \mid \cdot}(z_{ijk})$.
The updates for $\Omega$ are obtained in a similar fashion as before:
\begin{align*}
	\omega_{k,12}^{(l+1)}  & = -\left\{(s_{k, 22} + \lambda_k) \left(\Omega_{k, 11}^{(l+1)}\right)\inv + \diag(d_{k, 12})\right\}\inv s_{k, 12},  \\
	\omega_{k, 22}^{(l+1)} & = \frac{n_k}{s_{k,22} + \lambda_k} + \left(\omega_{k,12}^{(l+1)}\right)^t \Omega_{k, 11}\inv \omega_{k, 12}^{(l+1)}.
\end{align*}
This is the M-step for the multiple graphs setting. Inference is performed by
alternating between the E-step and M-step as before.

\section{Adding a prior for $\Sigma_0$} % TODO: put back \Sigma when you hand it back
As $\Sigma_0$ is meant to represent how similar pairs of graphs are,
we would like such information to be inferred from the data rather than to be imposed by us.
We expand the model in~\eqref{eq:mg-model} by specifying
\[\Sigma_0 \sim \textsc{W}\inv(\Psi, \nu),\]
where $\textsc{W}\inv(\Psi, \nu)$ is the \emph{inverse Wishart} distribution whose density is
\[f(\Sigma_0; \Psi, \nu) = \frac{\det(\Psi)^\frac{\nu}{2}}{2^\frac{\nu K}{2} \Gamma_K(\frac{\nu}{2})} \det(\Sigma_0)^{-\frac{\nu + K + 1}{2}} \exp\left\{-\frac{1}{2} \tr(\Psi \Sigma_0\inv)\right\},\]
with parameters $\Psi$, a positive definite $K \times K$ matrix, and $\nu > K - 1$ a scalar.
The $Q$ function in Equation~\eqref{eq:mg-joint} is now
\begin{align*}
	Q\left(\Omega, \theta, \Sigma_0 \mid \Omega\powl, \theta\powl, \Sigma_0\powl, Y\right) & =  \E_{z \mid \Omega\powl, \theta\powl, \Sigma\powl, Y}\left\{\log p(\Omega,  z, \theta, \Sigma_0 \mid Y) \mid \Omega\powl, \theta\powl, \Sigma_0\powl, Y\right\} \\
	                                                                                       & = Q\left(\Omega, \theta \mid \Omega\powl, \theta\powl, Y\right) - \frac{2(\nu + K + 1) + p(p-1)}{4} \log\det(\Sigma_0)                                            \\
	                                                                                       & - \frac{1}{2} \tr(\Psi \Sigma_0\inv) + \text{constant}.
\end{align*}
Adding this new term does not change the computations in the E-step. % TODO: Put the proof of it in the appendix
Let us now compute the posterior distribution of $\Sigma_0$ in our model,
\begin{align*}
	p(\Sigma_0 \mid Y, \Omega, z, \theta) & = \frac{p(Y \mid \Omega) p(\Omega \mid z) p(z \mid \theta) p(\theta \mid \Sigma_0) p(\Sigma_0)}{p(Y \mid \Omega) p(\Omega \mid z) p(z \mid \theta) p(\theta)} \\
	                                      & = \frac{p(\theta \mid \Sigma_0) p(\Sigma_0)}{p(\theta)}                                                                                                       \\
	                                      & = p(\Sigma_0 \mid \theta).
\end{align*}
Such simplifications are possible because $\Sigma_0$ appears last in the model.
Now, we compute an M-step for $\Sigma_0$.
We use the fact that the inverse Wishart distribution is conjugate to the multivariate Gaussian distribution.
That is, if we observe a sample $\{\theta_{ij}\}_{i,j=1,\dots,p}$ in which each element
is $\Nor_K (\theta_0, \Sigma_0)$ distributed, then the posterior is
\[\Sigma_0 \mid \{\theta_{ij}\}_{i,j=1,\dots,p} \sim \textsc{W}\inv\left(\Psi + \sum_{i < j} (\theta_{ij} - \theta_0)(\theta_{ij} - \theta_0)^t , \frac{p(p-1)}{2} + \nu\right).\]
This motivates the update step in which we simply set $\Sigma^{(l+1)}$ to the mode of the posterior distribution
\[\Sigma^{(l+1)} = \frac{\Psi + \sum_{i < j} (\theta_{ij} - \theta_0)(\theta_{ij} - \theta_0)^t}{\frac{p(p-1)}{2} + \nu + K + 1}.\]
In Chapter~\ref{chap:simualtions} we will perform simulations with and without this prior on $\Sigma$.


\chapter{Simulations}\label{chap:simualtions}

\section{Choosing hyperparameters}\label{sec:hyperparameters}
This section mainly concerns the choice of $v_0$ in the single-graph model from Chapter~\ref{chap:graphs},
whose performance is heavily influenced by the values of the
hyperparameters.
To assess the performance of a choice of hyperparameter we use the \emph{Area Under Curve} (AUC) metric \citep{hastie2009elements}.
Figure~\ref{fig:mean_auc} shows the mean AUC for
\begin{figure}[tb]
	\centering
	\includegraphics[width=.7\textwidth]{pictures/Rplot01.png}
	\caption{Mean AUC over ten replicates for different values of $v_0$, with a
		graph with 25 nodes and 100 samples.}\label{fig:mean_auc}
\end{figure}
changing values of $v_0$ in the single-graph setting from Chapter~\ref{chap:graphs}, fixing $v_1 = 100$.
The choice of $v_0$ represents the expected number of edges in the graph.
If $v_0$ is small, then only a few of the entries of $\Omega$ will be close
enough to zero to be absorbed in the ``spike'' of the spike-and-slab, which
leads to a large number of estimated edges. On the other hand if $v_0$ is
large, we estimate fewer edges, if any.
Because of the large performance improvement that some values of $v_0$ yield,
we propose two methods to choose $v_0$.

\subsection{Imposing a given sparsity level}\label{sect:imposing-sparsity}
Suppose that the proportion of edges $s \in [0,1]$ is known.
Then, since the parameter $\pi$ controls the prior sparsity of the graph, we
fix $a = 1$ and $b$ so that the mean of $\pi$ is $a / (a+b) = s$.
We then run the single-graph model over a grid of $v_0$ values and pick that
for which the estimated graph has edge density closest to the $s$ we imposed.
Figure~\ref{fig:auc} shows the result of choosing $v_0$ with this method.
We used the true sparsity of approximately 0.15 for this graph.
Even when using the true sparsity this method often yielded $v_0$ values
that were larger than the one who maximizes the AUC.\@
Because of this, we decided to use the method described in the next section.
\begin{figure}[tb]
	\begin{center}
		\includegraphics[width=0.95\textwidth]{pictures/auc1.pdf}
	\end{center}
	\caption{Area under curve (``AUC'') values for single-graph estimation as a function of $v_0$, with $n=50$, $p=20$.
		The red line represents the value for $v_0$ we would choose if we followed the method outlined in Section~\ref{sect:imposing-sparsity},
		and the orange line the one we would choose with the method in Section~\ref{sect:graphical-aic}.}
	\label{fig:auc}
\end{figure}

\subsection{Graphical AIC}\label{sect:graphical-aic}
The Akaike information criterion (AIC) in the graphical setting is given by the following formula
\[2|E| - \log\det(\Omega) + \tr(S \Omega),\]
where $S = YY^t$, $Y = [y_1, \dots, y_n]$, and $|E|$ is the number of edges in the estimated graph.
As we can see in Figure~\ref{fig:aic}, the value of $v_0$ that minimizes AIC is the one which maximizes the AUC.\@
\begin{figure}[tb]
	\begin{center}
		\includegraphics[width=0.95\textwidth]{pictures/auc2.pdf}
	\end{center}
	\caption{AIC values for single-graph estimation as a function of $v_0$, with $n=50$, $p=20$.
		The green line represents the value of $v_0$ for which the area under curve (``AUC'') is maximized.
	}
	\label{fig:aic}
\end{figure}
Although here we only show the result for one graph, using the AIC in this way generally gives
$v_0$ values that are very close to the best $v_0$.
Other formulas could have been used such as the Bayes information criterion (BIC), or the
extended BIC (eBIC) from \citet{foygel2010extended},
\[ |E| \log(n) - \log\det(\Omega) + \tr(S \Omega) + 4 |E| \gamma \log(p), \]
where $\gamma \in [0,1]$ is a penalty term for larger graphs.
However, we have found that the BIC and the eBIC, often chose $v_0$ values that are
too large due to the stronger penalization on the number of edges.
This often gave graphs that did not have any edges.
Therefore we chose to select $v_0$ based on the AIC.\@
In the multiple-graphs setting, we fit the single-graph model
to each graph and use the selected $v_0$ values.

\subsection{Challenges regarding the choice of other hyperparameters in the multiple-graphs setting}
Although the graphical AIC provides an effective method to choose $v_0$,
the question of choosing $\Sigma_0$ (or $\Psi$ and $\nu$ if the prior on $\Sigma_0$ is used)
is difficult to answer.
The choice of diagonal elements of $\Sigma_0$ represents the prior variance that
$\theta$ should have. First, it is difficult to choose the off-diagonal elements
since we do not know a priori what the graphs' degree of similitude is.
Second, the choice of the diagonal elements of $\Sigma_0$ represents the
prior variance of $\theta$. We will speculate later that if it is too large,
the fitting procedure is unable to pool information across the graphs.
Unlike what we do for $v_0$, to select a good value of $\Sigma_0$
it would be impractical to perform a grid search for each of the entries of $\Sigma_0$
as it would entail a search in a $K^2$-dimensional space and there is no guranteee that
the resulting matrix would be positive-definite.

\section{Performance in the multi-graph setting}

\subsection{Data generation}\label{ssect:data-generation}
To examine the effect of pooling, we created a method that produces new random graphs by swapping edges.
If we increase the number of swapped edges, we can create new graphs with increasing degree of disagreement.
Those graphs also have similar levels of sparsity,
and their corrsponding precision matrices satisfy the positive-definite
constraint.
Formally, the data for the multi-graph setting is obtained by first taking a graph
that was generated with the R package \texttt{huge} \citep{huge2020}.
Then we make a copy of the original graph,
and for each edge, we randomly and independently decide if it will be swapped, with some probability.
If yes, we move the edge over to a random pair of unconnected vertices.
We also swap the relevant entries in the precision matrix.
If the resulting precision matrix is not positive definite, we discard it and start over.
This process is repeated until we obtain the desired number of graphs.

\subsection{Comparison with single-graph methods}
For an estimate produced by the multi-graph model, $\hat\Omega, \hat\theta$, we use the formula in~\ref{eq:pdelta1} to obtain the
posterior edge inclusion probabilities $p_{ijk} = p(\delta_{ijk} \mid Y, \hat\Omega, \hat\theta)$, for $i,j=1\dots,p$, $k=1,\dots,K$.
Then we say that the estimate considers that an edge exists if the correspoding $p_{ijk} > 1/2$.
This choice of threshold may seem arbitrary, but the multi-graph model tends to
only give edge inclusion probabilities that are very close either to zero or one.
Therefore almost any choice of threshold give identical results.
Furthermore, choosing $1/2$ as threshold corresponds to the median probability
model defined by~\citet{barbieri2004optimal}.
To assess the performance of an estimate, we use the $F1$ score.
The $F1$ score is the harmonic mean between the \emph{precision} and \emph{recall} of an estimate,
where precision is the fraction of truly existing edges among those that our estimate considers existing,
and recall is the fraction of existing edges that our estimate detects.
We compare the results of our method with that of~\citet{mein2006} which for a given node $i$, computes
\[\hat{\theta}^{i, \lambda} = \argmin\displaylimits_{\theta : \theta_i = 0}\frac{1}{n} ||Y_i - Y \theta||_2^2 + \eta ||\theta||_1,\]
where $Y_i$ is the $i$-th column of $Y$, $\theta_j^i = -\omega_{ij}/\omega_{ii}$, and $\eta$ is a constant that controls the $l_1$ penalty term.
We run the algorithm described in Chapter~\ref{chap:multigraph} with $n=50$ and $p=20$.
Figure~\ref{fig:omega-mg} shows one estimate that the multi-graph method produced.
\begin{figure}[bt]
	\centering
	\includegraphics[width=0.95\textwidth]{pictures/mg1.pdf}
	\caption{The entries of the estimated $\hat \Omega_k$ matrix with the multi-graph modlel compared to the true $\Omega_k$ matrix, for $k = 1$ in
		a scatterplot with the multi-graph model. Points in red correspond to true positives, orange are true negatives, blue are false positives, and purple are false negatives. The points have been jittered slightly so that overlapping points are more easily discernable.}
	\label{fig:omega-mg}
\end{figure}
We see that although the true $\Omega$ only has zero or positive entries, our method estimated some negative entries,
parhaps due to the symmetric nature of the spike-and-slab prior, but more work is required to understand why such mistakes occur.

We perform a series of experiments. In the first, we compare our multi-graph model to the single-graph models.
The results are summarized in Table~\ref{tab:mg-more-graphs}.
\begin{table}[tb]
	\caption{Average F1 score for $n=50$, $p=20$, $\Sigma_0$ is 0.1 on the diagonal and 0.01 on the off
		diagonal elements, and $K=5$ graphs with $10\%$ disagreement.
		The leftmost two columns (M+B, and MG($K = 1$)) are single-graph methods.
		Column ``M+B'' denotes~\citet{mein2006} method.
		Columns ``MG'' denote the multigraph method from Chapter~\ref{chap:multigraph}.
		The rows correspond to how the graph was generated.
		Random means that each edge indipendently had a given probability of existing.
		Scale-free means that the graph was created by adding one node with one edge at a time, and the edge was connected to one existing node with probability proportional to its degree.
		Cluster means that the nodes were separated into groups and nodes in the same group had greater chance of being connected.
		Each entry shows the average over 30 runs, with its 95\% confidence interval. }
	\label{tab:mg-more-graphs}
	\begin{center}
		\begin{tabular}[c]{l|cccccc}
			\toprule
			Graph      & M+B             & MG($K = 1$)     & MG ($K = 5$)    \\
			\midrule
			Random     & 0.57 $\pm$ 0.01 & 0.68 $\pm$ 0.02 & 0.68 $\pm$ 0.02 \\
			\midrule
			Scale-free & 0.51 $\pm$ 0.01 & 0.69 $\pm$ 0.01 & 0.69 $\pm$ 0.01 \\
			\midrule
			Cluster    & 0.58 $\pm$ 0.01 & 0.61 $\pm$ 0.01 & 0.61 $\pm$ 0.01 \\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}
Although this is expected, we are reassured to see that the multi-graph model
performs better than the other single-graph method \citep{mein2006}.
However we are disappointed to see that pooling information across graphs
seems to have provided no benefit to the performance of the multi-graph model.
We think this is due to the choice of hyperparameters.
Figure~\ref{fig:sgvsmg} shows the estimated $\Omega$ from a joint multi-graph estimation against a one-by-one estimation.
On the vertical axis around zero, we would like to see a large number of blue dots (non-edges), which would indicate
that the joint estimation produces fewer false positives. Red dots on the horizontal axis around zero instead indicate that
the joint estimation produces more true positives.
\begin{figure}[tb]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{pictures/sg_vs_mg_omega1.pdf}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{pictures/sg_vs_mg_omega2.pdf}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{pictures/sg_vs_mg_omega3.pdf}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{pictures/sg_vs_mg_omega4.pdf}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{pictures/sg_vs_mg_omega5.pdf}
	\end{subfigure}

	\caption{Scatterplot of single-graph estimate against multi-graph estimate, for $n=50$, $p=20$, and $K=5$.}
	\label{fig:sgvsmg}
\end{figure}
The main difficulty when choosing $\Sigma_0$ is that if the diagonal terms are too large
the prior variance of $\theta$ is too large and hence no pooling happens.
On the other hand, if we set use the prior on $\Sigma_0$, it is equally difficult to choose
$\Psi$ and $\nu$ for similar reasons. Further work is needed to determine if it
is possible to find a value of $\Sigma_0$ such that pooling occurs.

\section{Examining the effect of pooling}\label{sect:perf-self}
In the second experiment we fix $K=5$ and increase the dissimilarity between graphs.
The results are summarized in Table~\ref{tab:mg-more-random}.
\begin{table}[ht]
	\caption{F1 scores as the dissimilarity between graphs increases, for $n=50$, $p=20$, and $K=4$ (refer to Subsection~\ref{ssect:data-generation}).
		In particular, if $prob = 0.0$, we obtain the same graph several times.
		The rows correspond to how the graph was generated.
		Random means that each edge indipendently had a given probability of existing.
		Scale-free means that the graph was created by adding one node with one edge at a time, and the edge was connected to one existing node with probability proportional to its degree.
		Each entry averaged over five runs, in which the same five arbitrarily chosen seeds were used.
	}
	\label{tab:mg-more-random}
	\begin{center}
		\begin{tabular}[c]{l|cccccc}
			\toprule
			Graph  & $prob = 0.05$ & $prob = 0.1$ & $prob = 0.2$ & $prob = 0.5$ & $prob = 1.0$ \\
			\midrule
			Random & 0.634         & 0.646        & 0.638        & 0.656        & 0.642        \\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}
Here we only look at the multi-graph model.
We see that in general the model performs similarly,
regardless of how different the graphs are from each other.
This is due to a bug in the code and will be fixed.


In the third and final experiment, we increase the number of graphs and see the effect on the performance.
% The method performs as we expect in the sense that the estimation becomes more accurate as the number of graphs increases.
% It is interesting to see that as the number of graphs increases we hit a plateau in terms of the accuracy of the estimation.
% This is maybe due to the graph generation procedure, since each edge has an equal chance of being swapped, there is not much ``structure'' form which to learn by including more graphs.
\section{Oddities regarding the likelihood, to be explored for the final version}
When doing our experiments we noticed that the $Q(\Omega, \theta, \Sigma_0)$ function from Chapter~\ref{chap:multigraph}
is not getting optimized. This is strange given that the algorithm does produce satisfactory results.
We have found that the objective function will increase very quickly in the beginning iterations,
but then it will decrease and settle at a lower value. This can be seen in Figure~\ref{fig:loglik-mg-1}.
In particular the decrease is sharper and reaches a lower point the larger the number of graphs we are using.
This happens even when we do not use a prior on $\Sigma_0$.
In the best case scenario it is simply a miscalculation in our code.
Otherwise, this could possibly be due once again to how we generate our data, or worse,
it could mean that the model is misspecified and some other approach would be more beneficial (although unlikely).
Unfortunately, we do not have a particularly convincing explanation of why this phenomenon happens.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=.45\textwidth]{pictures/loglik-mg-1.pdf}
		\includegraphics[width=.45\textwidth]{pictures/loglik-mg-2.pdf}
		\hfill
	\end{center}
	\caption{Evolution of log likelihood over iterations of fitting the multi-graph model, $p=20$, and $n=50$. On top we have $K=4$ graphs, on the bottom $K=15$.}
	\label{fig:loglik-mg-1}
\end{figure}


\chapter{Discussion and further work}
We have derived a conditional expectation maximization algorithm for inference on multiple graphs
which outperforms current implementations. Using Bayesian priors, we were able to pool information across
graphs to improve our estimates.
For future work, we would like to solve the strange inconsistencies found during the simulations, or at
least to have an explanation of why they happen.
Also we would like to apply our estimation procedure to a real dataset.
The code for the simulations can be found on
\href{https://github.com/jkasalt/pdm_summary}{\texttt{https://github.com/jkasalt/pdm\_summary}}.

For the final version of the report we will fix the code for Section~\ref{sect:perf-self}, and add the third experiment. Furthermore if time permits we will apply our model to a real dataset.

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography
\end{document}
